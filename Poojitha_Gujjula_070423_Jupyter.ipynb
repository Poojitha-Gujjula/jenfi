{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f061255",
   "metadata": {},
   "source": [
    "# Poojitha_Gujjula_070423"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a33f54",
   "metadata": {},
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed68d21",
   "metadata": {},
   "source": [
    "> **1.Load and read the train and test data**\n",
    "\n",
    "> **2.Data preprocessing of train and test data**\n",
    "\n",
    "> **3.Model:1 Baseline models -> Fitted multiple classification models using k-fold cross-validation on the training data(Imbalanced data)**\n",
    "     \n",
    "> **4.Model:2 Balanced the data and again fitted the models to check which model is performing better**\n",
    "\n",
    "> **5.Feature Selection: Information Value/Weight of Evidence Analysis**\n",
    "\n",
    "> **6.Model:3 Applied Clasification models on those selected IV features after balancing them**\n",
    "\n",
    "> **7.Feature Reduction: Lasso/Ridge Regularization, Variance Inflation Factor**\n",
    "\n",
    "> **8.Model:4 Applied models on features after VIF**\n",
    "\n",
    "> **9.Performance Metrics of Final Model and Interpretation**\n",
    "\n",
    "> **10.Predicting the probability of target being 1 on Out of Time data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14f5120",
   "metadata": {},
   "source": [
    "## 1) IMPORTING REQUIRED LIBRARIES & READING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "abd8f12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#to disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#import ensemble for bagging\n",
    "import sklearn.ensemble as ensemble \n",
    "\n",
    "#import randomforest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "#import adaboostclassifer\n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "\n",
    "#import Decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#import Scoring metric like f1_score, Classification Report\n",
    "from sklearn.metrics import f1_score,classification_report,roc_auc_score,accuracy_score,recall_score,precision_score\n",
    "\n",
    "#import cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#import Grid_Search cv\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d190f32",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7fa3c194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and read the train dataset\n",
    "df_train = pd.read_csv(r\"C:\\Users\\pooji\\Downloads\\Training_data_Jenfi_assessment_070423.csv\")\n",
    "#df_train()\n",
    "#Load and read the test dataset\n",
    "df_test = pd.read_csv(r\"C:\\Users\\pooji\\Downloads\\Test_data_Jenfi_assessment_070423.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d310fe00",
   "metadata": {},
   "source": [
    "> There are 151 independant variables and 1 dependant (target) variable with 326 data points (Observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "e2225894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(326, 153)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for no of rows and columns in the dataset\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "7ef6e799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      int64\n",
       "M1            float64\n",
       "M2            float64\n",
       "M3            float64\n",
       "M4            float64\n",
       "               ...   \n",
       "M148          float64\n",
       "M149          float64\n",
       "M150          float64\n",
       "M151          float64\n",
       "target          int64\n",
       "Length: 153, dtype: object"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking data types of each variables\n",
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "8d3dbfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#target class distribution\n",
    "df_train['target'].value_counts().plot(kind='bar')\n",
    "plt.title('Target Distribution')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62001220",
   "metadata": {},
   "source": [
    "> **We can see that class is imbalanced**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "f8b6404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_train= df_train.copy()\n",
    "df2_test= df_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d245bd",
   "metadata": {},
   "source": [
    "# Preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014d9dde",
   "metadata": {},
   "source": [
    "## Preprocessing of train data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7107cc34",
   "metadata": {},
   "source": [
    "### MISSISNG VALUE TREATMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f087681",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for null values\n",
    "null= round(df_train.isnull().sum()/len(df_train),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13e790e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     0\n",
       "M1             0\n",
       "M2             0\n",
       "M3             0\n",
       "M4             0\n",
       "              ..\n",
       "M148          17\n",
       "M149          51\n",
       "M150          22\n",
       "M151           5\n",
       "target         0\n",
       "Length: 153, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for infinity values\n",
    "np.isinf(df_train).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19547cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing infinity with null values\n",
    "df_train.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd63c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping unnamed column ad target\n",
    "df_train= df_train.drop([\"Unnamed: 0\",\"target\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "354adb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M1</th>\n",
       "      <th>M2</th>\n",
       "      <th>M3</th>\n",
       "      <th>M4</th>\n",
       "      <th>M5</th>\n",
       "      <th>M6</th>\n",
       "      <th>M7</th>\n",
       "      <th>M8</th>\n",
       "      <th>M9</th>\n",
       "      <th>M10</th>\n",
       "      <th>...</th>\n",
       "      <th>M142</th>\n",
       "      <th>M143</th>\n",
       "      <th>M144</th>\n",
       "      <th>M145</th>\n",
       "      <th>M146</th>\n",
       "      <th>M147</th>\n",
       "      <th>M148</th>\n",
       "      <th>M149</th>\n",
       "      <th>M150</th>\n",
       "      <th>M151</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.240789e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.142857</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>2.387105</td>\n",
       "      <td>5.720456e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>651246451.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.306454e+05</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>5.943793e+06</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.137554</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.426000</td>\n",
       "      <td>0.914831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.454261e+06</td>\n",
       "      <td>6138990.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.029878</td>\n",
       "      <td>1.694720e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>402015474.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.694206e+06</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>4.264630e+06</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.940661</td>\n",
       "      <td>2.000506</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.016467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.359738e+07</td>\n",
       "      <td>1272552.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.428571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.103578</td>\n",
       "      <td>1.852271e+07</td>\n",
       "      <td>16.0</td>\n",
       "      <td>124910611.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.640223e+06</td>\n",
       "      <td>6.714286</td>\n",
       "      <td>9.142857</td>\n",
       "      <td>1.321201e+07</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.007713</td>\n",
       "      <td>0.502552</td>\n",
       "      <td>1.157555</td>\n",
       "      <td>0.988912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.243215e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.285714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.714286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.050155e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1362655.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.029361e+08</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.504471</td>\n",
       "      <td>2.827495</td>\n",
       "      <td>0.670929</td>\n",
       "      <td>0.483688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.892245e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.285714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>400018121.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.154725e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.230383e+06</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.600029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             M1         M2   M3        M4   M5         M6        M7  \\\n",
       "0  2.240789e+08        0.0  0.0  8.142857  1.5   2.857143  2.387105   \n",
       "1  5.454261e+06  6138990.0  0.0  5.142857  NaN   0.285714  1.029878   \n",
       "2  2.359738e+07  1272552.0  0.0  3.428571  NaN   4.000000  1.103578   \n",
       "3  2.243215e+08        0.0  0.0  3.285714  NaN  10.714286       NaN   \n",
       "4  6.892245e+07        0.0  0.0  3.285714  1.0   1.857143  0.000000   \n",
       "\n",
       "             M8    M9          M10  ...          M142      M143      M144  \\\n",
       "0  5.720456e+07   0.0  651246451.0  ...  3.306454e+05  0.142857  1.428571   \n",
       "1  1.694720e+08   0.0  402015474.0  ...  2.694206e+06  0.285714  1.285714   \n",
       "2  1.852271e+07  16.0  124910611.0  ...  4.640223e+06  6.714286  9.142857   \n",
       "3  1.050155e+06   0.0    1362655.0  ...  0.000000e+00  0.000000  6.000000   \n",
       "4           NaN   0.0  400018121.0  ...  7.154725e+07  0.000000  0.142857   \n",
       "\n",
       "           M145       M146  M147      M148      M149      M150      M151  \n",
       "0  5.943793e+06   3.714286   8.0  1.137554       NaN  8.426000  0.914831  \n",
       "1  4.264630e+06   2.600000  10.0  0.940661  2.000506       NaN  1.016467  \n",
       "2  1.321201e+07   6.500000  13.0  1.007713  0.502552  1.157555  0.988912  \n",
       "3  7.029361e+08  11.500000   0.0  1.504471  2.827495  0.670929  0.483688  \n",
       "4  1.230383e+06   0.833333   2.0       NaN       NaN       NaN  0.600029  \n",
       "\n",
       "[5 rows x 151 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc83b229",
   "metadata": {},
   "source": [
    "## Feature Transformation/Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4818b4a",
   "metadata": {},
   "source": [
    "#### Scaling the data using Min-Max Scalar\n",
    "Performed MinMax scaling on the data before KNN imputation to ensure that all features are on the same scale, as KNN imputation's distance metric is affected by the scale of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5992ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M1</th>\n",
       "      <th>M2</th>\n",
       "      <th>M3</th>\n",
       "      <th>M4</th>\n",
       "      <th>M5</th>\n",
       "      <th>M6</th>\n",
       "      <th>M7</th>\n",
       "      <th>M8</th>\n",
       "      <th>M9</th>\n",
       "      <th>M10</th>\n",
       "      <th>...</th>\n",
       "      <th>M142</th>\n",
       "      <th>M143</th>\n",
       "      <th>M144</th>\n",
       "      <th>M145</th>\n",
       "      <th>M146</th>\n",
       "      <th>M147</th>\n",
       "      <th>M148</th>\n",
       "      <th>M149</th>\n",
       "      <th>M150</th>\n",
       "      <th>M151</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.028259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448374</td>\n",
       "      <td>0.004334</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.002694</td>\n",
       "      <td>0.002285</td>\n",
       "      <td>0.103029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020602</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.010070</td>\n",
       "      <td>0.070286</td>\n",
       "      <td>0.035422</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001820</td>\n",
       "      <td>0.000220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.012651</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.448374</td>\n",
       "      <td>0.002737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.111324</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.009063</td>\n",
       "      <td>0.069952</td>\n",
       "      <td>0.024796</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.003635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013946</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.448374</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003772</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>0.100170</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.003951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>0.066384</td>\n",
       "      <td>0.064451</td>\n",
       "      <td>0.071729</td>\n",
       "      <td>0.061989</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.028276</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448374</td>\n",
       "      <td>0.001749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.098879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042296</td>\n",
       "      <td>0.208745</td>\n",
       "      <td>0.109673</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.005137</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.017182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448374</td>\n",
       "      <td>0.001749</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.001751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.069349</td>\n",
       "      <td>0.007947</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>0.038856</td>\n",
       "      <td>0.094876</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.152836</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.060492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309161</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.005035</td>\n",
       "      <td>0.116967</td>\n",
       "      <td>0.044505</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>0.012586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.099298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008056</td>\n",
       "      <td>0.069117</td>\n",
       "      <td>0.019074</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>0.042778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448374</td>\n",
       "      <td>0.002509</td>\n",
       "      <td>0.217662</td>\n",
       "      <td>0.006871</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.114152</td>\n",
       "      <td>0.116071</td>\n",
       "      <td>0.075196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017499</td>\n",
       "      <td>0.039548</td>\n",
       "      <td>0.018127</td>\n",
       "      <td>0.257230</td>\n",
       "      <td>0.079473</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>0.015382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077543</td>\n",
       "      <td>0.069105</td>\n",
       "      <td>0.009537</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>0.028947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448423</td>\n",
       "      <td>0.001521</td>\n",
       "      <td>0.090014</td>\n",
       "      <td>0.003772</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.116438</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.104004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035947</td>\n",
       "      <td>0.014124</td>\n",
       "      <td>0.008056</td>\n",
       "      <td>0.175368</td>\n",
       "      <td>0.026703</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>326 rows × 151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           M1        M2        M3        M4        M5        M6        M7  \\\n",
       "0    0.028259  0.000000  0.448374  0.004334  0.166667  0.002694  0.002285   \n",
       "1    0.012651  0.000458  0.448374  0.002737       NaN  0.000269  0.000986   \n",
       "2    0.013946  0.000095  0.448374  0.001825       NaN  0.003772  0.001056   \n",
       "3    0.028276  0.000000  0.448374  0.001749       NaN  0.010104       NaN   \n",
       "4    0.017182  0.000000  0.448374  0.001749  0.111111  0.001751  0.000000   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "321  0.038856  0.094876       NaN  0.001141       NaN  0.002829  0.000796   \n",
       "322  0.012586  0.000000       NaN  0.000228       NaN  0.000808       NaN   \n",
       "323  0.042778  0.000000  0.448374  0.002509  0.217662  0.006871  0.000784   \n",
       "324  0.015382  0.000000       NaN  0.000380       NaN  0.000269       NaN   \n",
       "325  0.028947  0.000000  0.448423  0.001521  0.090014  0.003772       NaN   \n",
       "\n",
       "           M8        M9       M10  ...      M142      M143      M144  \\\n",
       "0    0.103029  0.000000  0.020602  ...  0.000098  0.001412  0.010070   \n",
       "1    0.111324  0.000000  0.012718  ...  0.000795  0.002825  0.009063   \n",
       "2    0.100170  0.142857  0.003951  ...  0.001369  0.066384  0.064451   \n",
       "3    0.098879  0.000000  0.000043  ...  0.000000  0.000000  0.042296   \n",
       "4         NaN  0.000000  0.012655  ...  0.021101  0.000000  0.001007   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "321  0.152836  0.053571  0.060492  ...  0.309161  0.029661  0.005035   \n",
       "322  0.099298  0.000000  0.001872  ...  0.000120  0.000000  0.008056   \n",
       "323  0.114152  0.116071  0.075196  ...  0.017499  0.039548  0.018127   \n",
       "324       NaN  0.000000       NaN  ...  0.000000  0.000000  0.077543   \n",
       "325  0.116438  0.062500  0.104004  ...  0.035947  0.014124  0.008056   \n",
       "\n",
       "         M145      M146    M147      M148      M149      M150      M151  \n",
       "0    0.070286  0.035422  0.5000  0.000006       NaN  0.001820  0.000220  \n",
       "1    0.069952  0.024796  0.6250  0.000005  0.003635       NaN  0.000244  \n",
       "2    0.071729  0.061989  0.8125  0.000006  0.000913  0.000250  0.000237  \n",
       "3    0.208745  0.109673  0.0000  0.000008  0.005137  0.000145  0.000116  \n",
       "4    0.069349  0.007947  0.1250       NaN       NaN       NaN  0.000144  \n",
       "..        ...       ...     ...       ...       ...       ...       ...  \n",
       "321  0.116967  0.044505  0.7500  0.000006  0.000958  0.000074  0.000405  \n",
       "322  0.069117  0.019074  0.2500  0.000006       NaN       NaN  0.000240  \n",
       "323  0.257230  0.079473  0.5000  0.000004  0.001783  0.000243  0.000202  \n",
       "324  0.069105  0.009537  0.0000  0.000000  0.001816       NaN  0.000000  \n",
       "325  0.175368  0.026703  0.6250  0.000006  0.001812  0.000196  0.000237  \n",
       "\n",
       "[326 rows x 151 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import module\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scale features\n",
    "scaler = MinMaxScaler()\n",
    "model=scaler.fit(df_train)\n",
    "scaled_df_train=model.transform(df_train)\n",
    "\n",
    "# creating a data frame to put into KNN imputer\n",
    "df_train=pd.DataFrame(scaled_df_train,columns=df_train.columns)\n",
    "df_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ee3f91",
   "metadata": {},
   "source": [
    "### IMPUTING NULL VALUES USING KNN IMPUTATION WITH K=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24b59ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using KNN imputer to impute the data\n",
    "from sklearn.impute import KNNImputer\n",
    "import pandas as pd\n",
    "\n",
    "# Create an instance of the KNNImputer class\n",
    "imputer = KNNImputer(n_neighbors=1)\n",
    "\n",
    "# Fit and transform the DataFrame to impute missing values\n",
    "df_train_imputed = imputer.fit_transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f9162b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_imputed = pd.DataFrame(df_train_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "530639a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverse transform of Knn Imputed File\n",
    "df_train_imputed=pd.DataFrame(scaler.inverse_transform(df_train_imputed),columns=df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8111a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce55db6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'M10',\n",
       "       ...\n",
       "       'M143', 'M144', 'M145', 'M146', 'M147', 'M148', 'M149', 'M150', 'M151',\n",
       "       'target'],\n",
       "      dtype='object', length=152)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df1_train.target\n",
    "\n",
    "join= [df_train_imputed,target] \n",
    "df_train_imputed = pd.concat(join,axis=1,join='inner')  \n",
    "df_train_imputed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "36917fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a794b1",
   "metadata": {},
   "source": [
    "## Preprocessing of test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db127395",
   "metadata": {},
   "source": [
    "### MISSISNG VALUE TREATMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0bbd2780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>M72</th>\n",
       "      <td>255</td>\n",
       "      <td>78.220859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M5</th>\n",
       "      <td>242</td>\n",
       "      <td>74.233129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M36</th>\n",
       "      <td>239</td>\n",
       "      <td>73.312883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M49</th>\n",
       "      <td>229</td>\n",
       "      <td>70.245399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M46</th>\n",
       "      <td>206</td>\n",
       "      <td>63.190184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Total    Percent\n",
       "M72    255  78.220859\n",
       "M5     242  74.233129\n",
       "M36    239  73.312883\n",
       "M49    229  70.245399\n",
       "M46    206  63.190184"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking missing data\n",
    "total = df_train.isnull().sum().sort_values(ascending = False)\n",
    "percent = (df_train.isnull().sum()/df_train.isnull().count()*100).sort_values(ascending = False)\n",
    "null = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "null.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf4a46d",
   "metadata": {},
   "source": [
    "> Columns which we are dropping from train data need to be dropped in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7a0914bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     0\n",
       "M1             0\n",
       "M2             0\n",
       "M3             0\n",
       "M4             0\n",
       "              ..\n",
       "M148           5\n",
       "M149          10\n",
       "M150           9\n",
       "M151           0\n",
       "target         0\n",
       "Length: 153, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for infinity values\n",
    "np.isinf(df_test).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9a729b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing infinity with null values\n",
    "df_test.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fd07a35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test= df_test.drop([\"Unnamed: 0\",\"target\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9535b069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M1</th>\n",
       "      <th>M2</th>\n",
       "      <th>M3</th>\n",
       "      <th>M4</th>\n",
       "      <th>M5</th>\n",
       "      <th>M6</th>\n",
       "      <th>M7</th>\n",
       "      <th>M8</th>\n",
       "      <th>M9</th>\n",
       "      <th>M10</th>\n",
       "      <th>M11</th>\n",
       "      <th>M12</th>\n",
       "      <th>M13</th>\n",
       "      <th>M14</th>\n",
       "      <th>M15</th>\n",
       "      <th>M16</th>\n",
       "      <th>M17</th>\n",
       "      <th>M18</th>\n",
       "      <th>M19</th>\n",
       "      <th>M20</th>\n",
       "      <th>M21</th>\n",
       "      <th>M22</th>\n",
       "      <th>M23</th>\n",
       "      <th>M24</th>\n",
       "      <th>M25</th>\n",
       "      <th>M26</th>\n",
       "      <th>M27</th>\n",
       "      <th>M28</th>\n",
       "      <th>M29</th>\n",
       "      <th>M30</th>\n",
       "      <th>M31</th>\n",
       "      <th>M32</th>\n",
       "      <th>M33</th>\n",
       "      <th>M34</th>\n",
       "      <th>M35</th>\n",
       "      <th>M36</th>\n",
       "      <th>M37</th>\n",
       "      <th>M38</th>\n",
       "      <th>M39</th>\n",
       "      <th>M40</th>\n",
       "      <th>M41</th>\n",
       "      <th>M42</th>\n",
       "      <th>M43</th>\n",
       "      <th>M44</th>\n",
       "      <th>M45</th>\n",
       "      <th>M46</th>\n",
       "      <th>M47</th>\n",
       "      <th>M48</th>\n",
       "      <th>M49</th>\n",
       "      <th>M50</th>\n",
       "      <th>M51</th>\n",
       "      <th>M52</th>\n",
       "      <th>M53</th>\n",
       "      <th>M54</th>\n",
       "      <th>M55</th>\n",
       "      <th>M56</th>\n",
       "      <th>M57</th>\n",
       "      <th>M58</th>\n",
       "      <th>M59</th>\n",
       "      <th>M60</th>\n",
       "      <th>M61</th>\n",
       "      <th>M62</th>\n",
       "      <th>M63</th>\n",
       "      <th>M64</th>\n",
       "      <th>M65</th>\n",
       "      <th>...</th>\n",
       "      <th>M87</th>\n",
       "      <th>M88</th>\n",
       "      <th>M89</th>\n",
       "      <th>M90</th>\n",
       "      <th>M91</th>\n",
       "      <th>M92</th>\n",
       "      <th>M93</th>\n",
       "      <th>M94</th>\n",
       "      <th>M95</th>\n",
       "      <th>M96</th>\n",
       "      <th>M97</th>\n",
       "      <th>M98</th>\n",
       "      <th>M99</th>\n",
       "      <th>M100</th>\n",
       "      <th>M101</th>\n",
       "      <th>M102</th>\n",
       "      <th>M103</th>\n",
       "      <th>M104</th>\n",
       "      <th>M105</th>\n",
       "      <th>M106</th>\n",
       "      <th>M107</th>\n",
       "      <th>M108</th>\n",
       "      <th>M109</th>\n",
       "      <th>M110</th>\n",
       "      <th>M111</th>\n",
       "      <th>M112</th>\n",
       "      <th>M113</th>\n",
       "      <th>M114</th>\n",
       "      <th>M115</th>\n",
       "      <th>M116</th>\n",
       "      <th>M117</th>\n",
       "      <th>M118</th>\n",
       "      <th>M119</th>\n",
       "      <th>M120</th>\n",
       "      <th>M121</th>\n",
       "      <th>M122</th>\n",
       "      <th>M123</th>\n",
       "      <th>M124</th>\n",
       "      <th>M125</th>\n",
       "      <th>M126</th>\n",
       "      <th>M127</th>\n",
       "      <th>M128</th>\n",
       "      <th>M129</th>\n",
       "      <th>M130</th>\n",
       "      <th>M131</th>\n",
       "      <th>M132</th>\n",
       "      <th>M133</th>\n",
       "      <th>M134</th>\n",
       "      <th>M135</th>\n",
       "      <th>M136</th>\n",
       "      <th>M137</th>\n",
       "      <th>M138</th>\n",
       "      <th>M139</th>\n",
       "      <th>M140</th>\n",
       "      <th>M141</th>\n",
       "      <th>M142</th>\n",
       "      <th>M143</th>\n",
       "      <th>M144</th>\n",
       "      <th>M145</th>\n",
       "      <th>M146</th>\n",
       "      <th>M147</th>\n",
       "      <th>M148</th>\n",
       "      <th>M149</th>\n",
       "      <th>M150</th>\n",
       "      <th>M151</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.728364e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.399986e+06</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.571429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.774900e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.277520e+05</td>\n",
       "      <td>10853006.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.428571</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.378666e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4200000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.560000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115918.0</td>\n",
       "      <td>42017.0</td>\n",
       "      <td>5.853168e+13</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1516623.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.018036e+07</td>\n",
       "      <td>4.714286e+06</td>\n",
       "      <td>3</td>\n",
       "      <td>4.614815e+07</td>\n",
       "      <td>33000000.0</td>\n",
       "      <td>27746.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.571429</td>\n",
       "      <td>6086005.0</td>\n",
       "      <td>2.128075e+07</td>\n",
       "      <td>1</td>\n",
       "      <td>1.206034e+07</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>-6.443362e+06</td>\n",
       "      <td>2.177592e+13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.671429e+07</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>44000.0</td>\n",
       "      <td>32</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>28775.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.571429</td>\n",
       "      <td>42017.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9033544.0</td>\n",
       "      <td>8.408577e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.285714</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.796102e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.877500e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>1.626117e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.142857</td>\n",
       "      <td>1.619399e+07</td>\n",
       "      <td>3.215477e+06</td>\n",
       "      <td>4.442623</td>\n",
       "      <td>1.495751e+07</td>\n",
       "      <td>4747051.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.856550e+06</td>\n",
       "      <td>1.665730</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>190000.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>4665875.0</td>\n",
       "      <td>116433433.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.142857</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>5.714286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>5.551383e+06</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.032424</td>\n",
       "      <td>0.732185</td>\n",
       "      <td>0.985438</td>\n",
       "      <td>0.048041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.338423e+06</td>\n",
       "      <td>1526173.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.857143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.428571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.636418e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.368034e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.300000e+07</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.764371e+06</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.803452e+06</td>\n",
       "      <td>100000000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.128205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>5.139768e+14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.925216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.507640e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2.749027e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1004341.0</td>\n",
       "      <td>5.966958e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.066667</td>\n",
       "      <td>4601980.0</td>\n",
       "      <td>6.142857e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>1.284518e+06</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-2.153286e+05</td>\n",
       "      <td>3.112605e+11</td>\n",
       "      <td>-1393400.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-384.0</td>\n",
       "      <td>65</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>5195570.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>0.989342</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.262449e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.510695e+06</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>3.667040e+06</td>\n",
       "      <td>1.389384e+11</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.553571</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.750420e+06</td>\n",
       "      <td>1.599992e+06</td>\n",
       "      <td>1.024418e+07</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>2.819764e+06</td>\n",
       "      <td>5.955433e+06</td>\n",
       "      <td>6.962011e+06</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>2.471720e+06</td>\n",
       "      <td>1.039070e+06</td>\n",
       "      <td>1.727273</td>\n",
       "      <td>1.732082e+06</td>\n",
       "      <td>1528938.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.482270e+06</td>\n",
       "      <td>1.940000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>2476329.0</td>\n",
       "      <td>0.584980</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2476329.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1656758.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.553804e+07</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>2.197055e+06</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.933704</td>\n",
       "      <td>0.996820</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.034571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.778643e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.180000e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.428571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.428571</td>\n",
       "      <td>2.285714</td>\n",
       "      <td>7.857143e+03</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.296667e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.634146</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1483401.0</td>\n",
       "      <td>1016224.0</td>\n",
       "      <td>5.155113e+15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.084429e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.674969e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>299000.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45029428.0</td>\n",
       "      <td>2.266667</td>\n",
       "      <td>178852812.0</td>\n",
       "      <td>8.644455e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>1.737371e+09</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>6.808786e+06</td>\n",
       "      <td>1.837372e+15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.571429e+07</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>55000.0</td>\n",
       "      <td>60</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>67050631.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.142857</td>\n",
       "      <td>1016224.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30812829.0</td>\n",
       "      <td>8.002437e+07</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>4.904116e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.740487e+07</td>\n",
       "      <td>4.271429e+04</td>\n",
       "      <td>3.555405e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.546802e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.498000e+05</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>3.666282e+07</td>\n",
       "      <td>2.748350e+07</td>\n",
       "      <td>1.620690</td>\n",
       "      <td>1.566483e+07</td>\n",
       "      <td>6625759.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>1.126063e+06</td>\n",
       "      <td>3.616822</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>20189428.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>82483401.0</td>\n",
       "      <td>12136000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>75512523.0</td>\n",
       "      <td>9.131163e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.070932e+07</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.685500</td>\n",
       "      <td>1.111475</td>\n",
       "      <td>0.910898</td>\n",
       "      <td>1.730498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.036687e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.714286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.079382e+07</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.140875e+08</td>\n",
       "      <td>31998505.0</td>\n",
       "      <td>5.007902e+08</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.142857</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.571429</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>4.640077e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.422081e+07</td>\n",
       "      <td>43854895.0</td>\n",
       "      <td>20000000.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.344681</td>\n",
       "      <td>134000132.0</td>\n",
       "      <td>86230848.0</td>\n",
       "      <td>2410701.0</td>\n",
       "      <td>3.171263e+15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.379928</td>\n",
       "      <td>648.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.142857</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.742447e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>1.837986e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1819952.0</td>\n",
       "      <td>1.593748e+08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>224474533.0</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>170943295.0</td>\n",
       "      <td>5.493165e+07</td>\n",
       "      <td>1</td>\n",
       "      <td>8.045639e+06</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>1.043962e+06</td>\n",
       "      <td>3.894369e+15</td>\n",
       "      <td>-192370999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1204892.0</td>\n",
       "      <td>18</td>\n",
       "      <td>6.571429</td>\n",
       "      <td>402639612.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7753089.0</td>\n",
       "      <td>1.360041</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>6.783680e+07</td>\n",
       "      <td>183477771.0</td>\n",
       "      <td>1.418263e+08</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.517720e+08</td>\n",
       "      <td>5.484977e+15</td>\n",
       "      <td>8.166667</td>\n",
       "      <td>7.285714</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>35.027778</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>2.384840e+08</td>\n",
       "      <td>2.420915e+08</td>\n",
       "      <td>2.320682e+08</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.208207e+08</td>\n",
       "      <td>1.964553e+08</td>\n",
       "      <td>1.757749e+08</td>\n",
       "      <td>6.285714</td>\n",
       "      <td>1.104968e+08</td>\n",
       "      <td>1.571415e+08</td>\n",
       "      <td>8.403846</td>\n",
       "      <td>9.255668e+07</td>\n",
       "      <td>104001178.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.142857</td>\n",
       "      <td>9.670447e+07</td>\n",
       "      <td>0.414894</td>\n",
       "      <td>11</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>202626940.5</td>\n",
       "      <td>19.728836</td>\n",
       "      <td>6.571429</td>\n",
       "      <td>202626940.5</td>\n",
       "      <td>43366698.0</td>\n",
       "      <td>135288545.0</td>\n",
       "      <td>7.571429</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.428571</td>\n",
       "      <td>365782641.0</td>\n",
       "      <td>1.606599e+08</td>\n",
       "      <td>8.285714</td>\n",
       "      <td>6.285714</td>\n",
       "      <td>1.237771e+08</td>\n",
       "      <td>8.166667</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.773008</td>\n",
       "      <td>2.326502</td>\n",
       "      <td>1.186048</td>\n",
       "      <td>0.923257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.713252e+07</td>\n",
       "      <td>1490847.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.132774e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.086385e+09</td>\n",
       "      <td>8319737.0</td>\n",
       "      <td>7.630404e+09</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>43.0</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.609798e+08</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.571429</td>\n",
       "      <td>2.279258e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>201000000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3.081633</td>\n",
       "      <td>2553609.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>870422.0</td>\n",
       "      <td>3.032327e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.997680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.096504e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3.042034e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>231645.0</td>\n",
       "      <td>3.449121e+07</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1462991.0</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>16810432.0</td>\n",
       "      <td>2.542932e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>2.511726e+06</td>\n",
       "      <td>7.714286</td>\n",
       "      <td>-1.285714e+05</td>\n",
       "      <td>1.353842e+14</td>\n",
       "      <td>-900000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>6516087.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.714286</td>\n",
       "      <td>1051991.0</td>\n",
       "      <td>0.984029</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>1.025313e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.163516e+08</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>2.826564e+06</td>\n",
       "      <td>1.507321e+17</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.410714</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.804674e+08</td>\n",
       "      <td>1.141999e+09</td>\n",
       "      <td>3.283688e+08</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>5.662246e+08</td>\n",
       "      <td>1.413068e+08</td>\n",
       "      <td>1.452059e+09</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.960297e+08</td>\n",
       "      <td>1.130638e+08</td>\n",
       "      <td>2.657895</td>\n",
       "      <td>1.011828e+06</td>\n",
       "      <td>5883461.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>5.026465e+07</td>\n",
       "      <td>3.123377</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>82935353.0</td>\n",
       "      <td>2.183983</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>82935353.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1951991.0</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>3782148.0</td>\n",
       "      <td>2.406444e+06</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.477728e+07</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.029067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.044883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             M1         M2            M3        M4  M5        M6  M7            M8   M9           M10         M11           M12       M13   M14  M15       M16   M17       M18       M19           M20  M21       M22           M23          M24          M25  M26  M27       M28          M29         M30        M31  \\\n",
       "0  3.728364e+06        0.0  1.399986e+06  2.857143 NaN  6.571429 NaN  2.774900e+04  0.0  1.277520e+05  10853006.0           NaN  2.857143   2.0    0  0.000000   8.0  7.428571  3.857143  0.000000e+00  0.0  0.000000  2.378666e+07          0.0    4200000.0    3    4  1.560000          0.0    115918.0    42017.0   \n",
       "1  2.338423e+06  1526173.0           NaN  1.857143 NaN  3.428571 NaN  1.636418e+06  0.0  2.368034e+07         0.0  6.300000e+07  0.428571   7.0    2  2.000000   3.0  1.285714  2.000000  8.764371e+06  2.0  2.000000  4.803452e+06  100000000.0          NaN    2    2  2.128205          0.0         NaN  1000000.0   \n",
       "2  5.778643e+07        0.0  0.000000e+00  2.000000 NaN  1.714286 NaN           NaN  0.0  5.180000e+05         0.0  0.000000e+00  3.428571   1.0    0  0.000000   6.0  5.428571  2.285714  7.857143e+03  2.0  0.000000  2.296667e+08          0.0          NaN    0    1  3.634146          0.0   1483401.0  1016224.0   \n",
       "3  2.036687e+08        0.0  0.000000e+00  2.000000 NaN  2.714286 NaN  9.079382e+07  9.0  5.140875e+08  31998505.0  5.007902e+08  1.428571  20.0    0  2.142857  21.0  3.571429  2.571429  4.640077e+07  0.0  1.000000  6.422081e+07   43854895.0   20000000.0    9    0  0.344681  134000132.0  86230848.0  2410701.0   \n",
       "4  6.713252e+07  1490847.0  0.000000e+00  2.000000 NaN  6.000000 NaN  2.132774e+08  0.0  6.086385e+09   8319737.0  7.630404e+09  4.000000  39.0    2  2.428571  43.0  4.571429  6.000000  2.609798e+08  3.0  3.571429  2.279258e+07          0.0  201000000.0    4    1  3.081633    2553609.0         NaN   870422.0   \n",
       "\n",
       "            M32  M33       M34    M35  M36       M37  M38  M39        M40   M41   M42   M43  M44           M45           M46  M47           M48         M49        M50           M51  M52       M53          M54        M55          M56           M57  M58           M59       M60           M61           M62          M63  \\\n",
       "0  5.853168e+13  4.0       NaN    6.0  NaN  0.285714  0.0  NaN  1516623.5   NaN   NaN   1.0  0.0  5.018036e+07  4.714286e+06    3  4.614815e+07  33000000.0    27746.0  0.000000e+00  1.0  3.000000          0.0  20.571429    6086005.0  2.128075e+07    1  1.206034e+07  7.000000 -6.443362e+06  2.177592e+13          NaN   \n",
       "1  5.139768e+14  NaN  0.925216    0.0  NaN  0.714286  0.0  NaN        NaN   6.0  32.0   9.0  8.0  2.507640e+07           NaN    1  2.749027e+06         NaN  1004341.0  5.966958e+06  NaN  2.000000          0.0  11.066667    4601980.0  6.142857e+06    0  1.284518e+06  4.000000 -2.153286e+05  3.112605e+11   -1393400.0   \n",
       "2  5.155113e+15  1.0       NaN    NaN  NaN  0.000000  0.0  NaN        NaN   NaN   4.0   0.0  4.0  7.084429e+06  0.000000e+00    1  1.674969e+09         NaN   299000.0  0.000000e+00  0.0       NaN   45029428.0   2.266667  178852812.0  8.644455e+06    1  1.737371e+09  1.714286  6.808786e+06  1.837372e+15          NaN   \n",
       "3  3.171263e+15  NaN  0.379928  648.0  NaN  4.142857  7.0  0.0        NaN  15.0  18.0  10.0  9.0  6.742447e+07           NaN    7  1.837986e+08         NaN  1819952.0  1.593748e+08  1.0  0.111111  224474533.0   7.100000  170943295.0  5.493165e+07    1  8.045639e+06  2.571429  1.043962e+06  3.894369e+15 -192370999.0   \n",
       "4  3.032327e+18  NaN  2.997680    0.0  NaN  1.571429  1.0  0.0        NaN  24.0  69.0  12.0  7.0  1.096504e+09           NaN    0  3.042034e+08         NaN   231645.0  3.449121e+07  2.0  4.000000    1462991.0  13.600000   16810432.0  2.542932e+07    0  2.511726e+06  7.714286 -1.285714e+05  1.353842e+14    -900000.0   \n",
       "\n",
       "            M64   M65  ...        M87  M88       M89          M90  M91   M92       M93        M94       M95       M96           M97          M98           M99      M100          M101          M102      M103      M104      M105  M106       M107  M108  M109          M110          M111          M112  M113      M114  \\\n",
       "0  1.671429e+07   8.0  ...    44000.0   32  4.571429      28775.0  0.0   1.0  3.571429    42017.0       NaN       NaN           NaN    9033544.0  8.408577e+05  0.000000  0.000000e+00           NaN       NaN  3.285714  1.833333   0.0        NaN   0.5   1.0  2.796102e+06  0.000000e+00  2.877500e+04   NaN  1.142857   \n",
       "1           NaN   5.0  ...     -384.0   65  0.714286    5195570.5  2.0   8.0  1.142857  1000000.0  0.989342  0.750000  1.262449e+06          0.0  1.510695e+06  0.571429  3.667040e+06  1.389384e+11  1.200000  1.714286  0.666667   2.0   2.553571   1.5   1.0  1.750420e+06  1.599992e+06  1.024418e+07  26.0  0.571429   \n",
       "2  3.571429e+07  13.0  ...    55000.0   60  1.714286   67050631.0  1.0   1.0  2.142857  1016224.0       NaN       NaN           NaN   30812829.0  8.002437e+07  0.142857  4.904116e+07           NaN       NaN  1.000000  1.500000   0.0   0.250000   0.5   NaN  5.740487e+07  4.271429e+04  3.555405e+07   NaN  0.428571   \n",
       "3           NaN  10.0  ... -1204892.0   18  6.571429  402639612.5  0.0  10.0  6.000000  7753089.0  1.360041  9.333333  6.783680e+07  183477771.0  1.418263e+08  8.000000  2.517720e+08  5.484977e+15  8.166667  7.285714  7.000000  19.0  35.027778   9.0  10.5  2.384840e+08  2.420915e+08  2.320682e+08  21.0  8.000000   \n",
       "4           NaN  19.0  ...        0.0   55  1.285714    6516087.0  2.0  12.0  2.714286  1051991.0  0.984029  1.666667  1.025313e+08          0.0  1.163516e+08  1.571429  2.826564e+06  1.507321e+17  2.833333  3.857143  1.600000   0.0   6.410714   2.0   1.0  8.804674e+08  1.141999e+09  3.283688e+08  42.0  0.142857   \n",
       "\n",
       "           M115          M116          M117      M118          M119          M120      M121          M122         M123  M124      M125          M126      M127  M128  M129  M130      M131         M132       M133      M134         M135         M136         M137      M138      M139      M140         M141          M142  \\\n",
       "0  1.626117e+07  0.000000e+00  0.000000e+00  2.142857  1.619399e+07  3.215477e+06  4.442623  1.495751e+07    4747051.0  21.0  0.000000  1.856550e+06  1.665730     1   0.0   0.0  2.428571     190000.0   0.500000  0.142857    4665875.0  116433433.0          NaN  2.142857  1.714286  5.714286          0.0  0.000000e+00   \n",
       "1  2.819764e+06  5.955433e+06  6.962011e+06  0.571429  2.471720e+06  1.039070e+06  1.727273  1.732082e+06    1528938.0   0.0  1.000000  6.482270e+06  1.940000     0   1.0   2.0  1.428571    2476329.0   0.584980  1.000000    2476329.0          0.0    1656758.0  0.571429  1.285714  1.571429          0.0  1.553804e+07   \n",
       "2  1.546802e+07  0.000000e+00  4.498000e+05  0.285714  3.666282e+07  2.748350e+07  1.620690  1.566483e+07    6625759.0  22.0  1.285714  1.126063e+06  3.616822     0   1.0   0.0  2.571429   20189428.0   0.500000  0.000000   82483401.0   12136000.0          NaN  1.285714  1.428571  0.714286   75512523.0  9.131163e+06   \n",
       "3  1.208207e+08  1.964553e+08  1.757749e+08  6.285714  1.104968e+08  1.571415e+08  8.403846  9.255668e+07  104001178.0   1.0  8.142857  9.670447e+07  0.414894    11  13.0   7.0  8.000000  202626940.5  19.728836  6.571429  202626940.5   43366698.0  135288545.0  7.571429  7.000000  7.428571  365782641.0  1.606599e+08   \n",
       "4  5.662246e+08  1.413068e+08  1.452059e+09  2.000000  1.960297e+08  1.130638e+08  2.657895  1.011828e+06    5883461.0   0.0  1.428571  5.026465e+07  3.123377     0   0.0   0.0  1.714286   82935353.0   2.183983  1.142857   82935353.0          0.0    1951991.0  1.428571  2.428571  1.714286    3782148.0  2.406444e+06   \n",
       "\n",
       "       M143      M144          M145      M146  M147      M148      M149      M150      M151  \n",
       "0  0.142857  1.714286  5.551383e+06  2.571429   1.0  1.032424  0.732185  0.985438  0.048041  \n",
       "1  0.714286  0.285714  2.197055e+06  1.500000  11.0  0.933704  0.996820       NaN  1.034571  \n",
       "2  0.000000  1.000000  1.070932e+07  0.500000   1.0  3.685500  1.111475  0.910898  1.730498  \n",
       "3  8.285714  6.285714  1.237771e+08  8.166667  10.0  0.773008  2.326502  1.186048  0.923257  \n",
       "4  0.571429  1.000000  7.477728e+07  3.000000  10.0  1.029067       NaN       NaN  1.044883  \n",
       "\n",
       "[5 rows x 151 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71754be2",
   "metadata": {},
   "source": [
    "## Feature Transformation/Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82123e47",
   "metadata": {},
   "source": [
    "#### Scaling the data using Min-Max Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "93fb2db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M1</th>\n",
       "      <th>M2</th>\n",
       "      <th>M3</th>\n",
       "      <th>M4</th>\n",
       "      <th>M5</th>\n",
       "      <th>M6</th>\n",
       "      <th>M7</th>\n",
       "      <th>M8</th>\n",
       "      <th>M9</th>\n",
       "      <th>M10</th>\n",
       "      <th>M11</th>\n",
       "      <th>M12</th>\n",
       "      <th>M13</th>\n",
       "      <th>M14</th>\n",
       "      <th>M15</th>\n",
       "      <th>M16</th>\n",
       "      <th>M17</th>\n",
       "      <th>M18</th>\n",
       "      <th>M19</th>\n",
       "      <th>M20</th>\n",
       "      <th>M21</th>\n",
       "      <th>M22</th>\n",
       "      <th>M23</th>\n",
       "      <th>M24</th>\n",
       "      <th>M25</th>\n",
       "      <th>M26</th>\n",
       "      <th>M27</th>\n",
       "      <th>M28</th>\n",
       "      <th>M29</th>\n",
       "      <th>M30</th>\n",
       "      <th>M31</th>\n",
       "      <th>M32</th>\n",
       "      <th>M33</th>\n",
       "      <th>M34</th>\n",
       "      <th>M35</th>\n",
       "      <th>M36</th>\n",
       "      <th>M37</th>\n",
       "      <th>M38</th>\n",
       "      <th>M39</th>\n",
       "      <th>M40</th>\n",
       "      <th>M41</th>\n",
       "      <th>M42</th>\n",
       "      <th>M43</th>\n",
       "      <th>M44</th>\n",
       "      <th>M45</th>\n",
       "      <th>M46</th>\n",
       "      <th>M47</th>\n",
       "      <th>M48</th>\n",
       "      <th>M49</th>\n",
       "      <th>M50</th>\n",
       "      <th>M51</th>\n",
       "      <th>M52</th>\n",
       "      <th>M53</th>\n",
       "      <th>M54</th>\n",
       "      <th>M55</th>\n",
       "      <th>M56</th>\n",
       "      <th>M57</th>\n",
       "      <th>M58</th>\n",
       "      <th>M59</th>\n",
       "      <th>M60</th>\n",
       "      <th>M61</th>\n",
       "      <th>M62</th>\n",
       "      <th>M63</th>\n",
       "      <th>M64</th>\n",
       "      <th>M65</th>\n",
       "      <th>...</th>\n",
       "      <th>M87</th>\n",
       "      <th>M88</th>\n",
       "      <th>M89</th>\n",
       "      <th>M90</th>\n",
       "      <th>M91</th>\n",
       "      <th>M92</th>\n",
       "      <th>M93</th>\n",
       "      <th>M94</th>\n",
       "      <th>M95</th>\n",
       "      <th>M96</th>\n",
       "      <th>M97</th>\n",
       "      <th>M98</th>\n",
       "      <th>M99</th>\n",
       "      <th>M100</th>\n",
       "      <th>M101</th>\n",
       "      <th>M102</th>\n",
       "      <th>M103</th>\n",
       "      <th>M104</th>\n",
       "      <th>M105</th>\n",
       "      <th>M106</th>\n",
       "      <th>M107</th>\n",
       "      <th>M108</th>\n",
       "      <th>M109</th>\n",
       "      <th>M110</th>\n",
       "      <th>M111</th>\n",
       "      <th>M112</th>\n",
       "      <th>M113</th>\n",
       "      <th>M114</th>\n",
       "      <th>M115</th>\n",
       "      <th>M116</th>\n",
       "      <th>M117</th>\n",
       "      <th>M118</th>\n",
       "      <th>M119</th>\n",
       "      <th>M120</th>\n",
       "      <th>M121</th>\n",
       "      <th>M122</th>\n",
       "      <th>M123</th>\n",
       "      <th>M124</th>\n",
       "      <th>M125</th>\n",
       "      <th>M126</th>\n",
       "      <th>M127</th>\n",
       "      <th>M128</th>\n",
       "      <th>M129</th>\n",
       "      <th>M130</th>\n",
       "      <th>M131</th>\n",
       "      <th>M132</th>\n",
       "      <th>M133</th>\n",
       "      <th>M134</th>\n",
       "      <th>M135</th>\n",
       "      <th>M136</th>\n",
       "      <th>M137</th>\n",
       "      <th>M138</th>\n",
       "      <th>M139</th>\n",
       "      <th>M140</th>\n",
       "      <th>M141</th>\n",
       "      <th>M142</th>\n",
       "      <th>M143</th>\n",
       "      <th>M144</th>\n",
       "      <th>M145</th>\n",
       "      <th>M146</th>\n",
       "      <th>M147</th>\n",
       "      <th>M148</th>\n",
       "      <th>M149</th>\n",
       "      <th>M150</th>\n",
       "      <th>M151</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.165297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007310</td>\n",
       "      <td>0.136054</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142415</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.044358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.055411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.009709</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.127451</td>\n",
       "      <td>0.126168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020896</td>\n",
       "      <td>0.046154</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.215172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.615261e-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005942</td>\n",
       "      <td>0.002306</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.006796</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>0.400547</td>\n",
       "      <td>0.048872</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012223</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.004892</td>\n",
       "      <td>0.158576</td>\n",
       "      <td>0.462857</td>\n",
       "      <td>1.543597e-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009931</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559381</td>\n",
       "      <td>0.192547</td>\n",
       "      <td>0.054889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.046468</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.037829</td>\n",
       "      <td>0.021605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.131050</td>\n",
       "      <td>0.124341</td>\n",
       "      <td>0.022081</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012232</td>\n",
       "      <td>0.003789</td>\n",
       "      <td>0.018387</td>\n",
       "      <td>0.105363</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.003086</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.048174</td>\n",
       "      <td>0.002995</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.195025</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.082142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022255</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.068611</td>\n",
       "      <td>0.032930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>0.031802</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.010525</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.164408</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.088435</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.074303</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.044633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.053411</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.058252</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.128440</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.022059</td>\n",
       "      <td>0.065421</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.024096</td>\n",
       "      <td>0.134615</td>\n",
       "      <td>0.055487</td>\n",
       "      <td>0.010751</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.293546</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003410</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.852666e-08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007072</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.031915</td>\n",
       "      <td>0.211921</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.002969</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.005667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.401851</td>\n",
       "      <td>0.050445</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006576</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.090615</td>\n",
       "      <td>0.467850</td>\n",
       "      <td>2.205454e-10</td>\n",
       "      <td>0.998366</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559102</td>\n",
       "      <td>0.397516</td>\n",
       "      <td>0.008576</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.014870</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>3.013948e-04</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.007018</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>4.632732e-09</td>\n",
       "      <td>0.014660</td>\n",
       "      <td>0.019737</td>\n",
       "      <td>0.007856</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>0.007475</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.130504</td>\n",
       "      <td>0.125236</td>\n",
       "      <td>0.023574</td>\n",
       "      <td>0.155689</td>\n",
       "      <td>0.006116</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.018941</td>\n",
       "      <td>0.109153</td>\n",
       "      <td>0.007018</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.015274</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.001312</td>\n",
       "      <td>0.229642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.002295</td>\n",
       "      <td>0.012590</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.059172</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.005935</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.032930</td>\n",
       "      <td>0.001858</td>\n",
       "      <td>0.007704</td>\n",
       "      <td>0.003367</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.018551</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.014329</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.199866</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.037152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.053411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.093137</td>\n",
       "      <td>0.074766</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.024096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097537</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010204</td>\n",
       "      <td>0.501262</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>0.003465</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.049165</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.400910</td>\n",
       "      <td>0.048872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005041</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.704746</td>\n",
       "      <td>0.038835</td>\n",
       "      <td>0.473481</td>\n",
       "      <td>1.302439e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.021219</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559451</td>\n",
       "      <td>0.366460</td>\n",
       "      <td>0.020583</td>\n",
       "      <td>0.008335</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.027881</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003396</td>\n",
       "      <td>0.010646</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.006906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011513</td>\n",
       "      <td>0.017677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.159576</td>\n",
       "      <td>0.124365</td>\n",
       "      <td>0.027273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004587</td>\n",
       "      <td>0.003604</td>\n",
       "      <td>0.018387</td>\n",
       "      <td>0.105608</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.006987</td>\n",
       "      <td>0.003471</td>\n",
       "      <td>0.013983</td>\n",
       "      <td>0.003137</td>\n",
       "      <td>0.001567</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015126</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.441284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032727</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004198</td>\n",
       "      <td>0.061566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013353</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.008576</td>\n",
       "      <td>0.041960</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011785</td>\n",
       "      <td>0.001946</td>\n",
       "      <td>0.006184</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.015978</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.010795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.293155</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.059858</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.013079</td>\n",
       "      <td>0.059307</td>\n",
       "      <td>0.008233</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.184466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137615</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.061275</td>\n",
       "      <td>0.084112</td>\n",
       "      <td>0.007869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067308</td>\n",
       "      <td>0.066598</td>\n",
       "      <td>0.004715</td>\n",
       "      <td>0.099502</td>\n",
       "      <td>0.138462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047542</td>\n",
       "      <td>0.037905</td>\n",
       "      <td>0.164819</td>\n",
       "      <td>0.008220</td>\n",
       "      <td>0.000307</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.150311e-09</td>\n",
       "      <td>4.984482e-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.041018</td>\n",
       "      <td>0.094595</td>\n",
       "      <td>0.800210</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.079787</td>\n",
       "      <td>0.119205</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.007984</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.010376</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.402940</td>\n",
       "      <td>0.090888</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.025131</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>0.006066</td>\n",
       "      <td>0.003171</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.003264</td>\n",
       "      <td>0.058252</td>\n",
       "      <td>0.468859</td>\n",
       "      <td>2.760560e-06</td>\n",
       "      <td>0.979335</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.080357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.551525</td>\n",
       "      <td>0.105590</td>\n",
       "      <td>0.078902</td>\n",
       "      <td>0.050068</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.078067</td>\n",
       "      <td>0.024010</td>\n",
       "      <td>4.143250e-04</td>\n",
       "      <td>0.096790</td>\n",
       "      <td>0.009735</td>\n",
       "      <td>0.020223</td>\n",
       "      <td>0.018867</td>\n",
       "      <td>0.098246</td>\n",
       "      <td>0.035452</td>\n",
       "      <td>1.828899e-04</td>\n",
       "      <td>0.099767</td>\n",
       "      <td>0.083882</td>\n",
       "      <td>0.082492</td>\n",
       "      <td>0.345455</td>\n",
       "      <td>0.102537</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.254165</td>\n",
       "      <td>0.259724</td>\n",
       "      <td>0.055989</td>\n",
       "      <td>0.125749</td>\n",
       "      <td>0.085627</td>\n",
       "      <td>0.028155</td>\n",
       "      <td>0.036649</td>\n",
       "      <td>0.201047</td>\n",
       "      <td>0.077193</td>\n",
       "      <td>0.021059</td>\n",
       "      <td>0.019846</td>\n",
       "      <td>0.096169</td>\n",
       "      <td>0.018533</td>\n",
       "      <td>0.024603</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.095798</td>\n",
       "      <td>0.019652</td>\n",
       "      <td>0.037149</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>0.203125</td>\n",
       "      <td>0.093333</td>\n",
       "      <td>0.101818</td>\n",
       "      <td>0.010312</td>\n",
       "      <td>0.077384</td>\n",
       "      <td>0.082734</td>\n",
       "      <td>0.010318</td>\n",
       "      <td>0.067727</td>\n",
       "      <td>0.026758</td>\n",
       "      <td>0.078635</td>\n",
       "      <td>0.090741</td>\n",
       "      <td>0.089194</td>\n",
       "      <td>0.076674</td>\n",
       "      <td>0.019209</td>\n",
       "      <td>0.089368</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.022492</td>\n",
       "      <td>0.101001</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.033444</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.005615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.205843</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.130031</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.080775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154861</td>\n",
       "      <td>0.054944</td>\n",
       "      <td>0.125440</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.368932</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.155963</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.196262</td>\n",
       "      <td>0.044259</td>\n",
       "      <td>0.036145</td>\n",
       "      <td>0.240385</td>\n",
       "      <td>0.058851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.061538</td>\n",
       "      <td>0.010204</td>\n",
       "      <td>0.425053</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002968</td>\n",
       "      <td>0.293959</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.176450e-08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015559</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.800210</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.456954</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.129841</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013508</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.400820</td>\n",
       "      <td>0.057965</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.008081</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.174757</td>\n",
       "      <td>0.467919</td>\n",
       "      <td>9.596829e-08</td>\n",
       "      <td>0.998416</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.160714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559105</td>\n",
       "      <td>0.335404</td>\n",
       "      <td>0.015437</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.035316</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>2.997761e-04</td>\n",
       "      <td>0.017284</td>\n",
       "      <td>0.014715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015478</td>\n",
       "      <td>0.019298</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>5.025978e-03</td>\n",
       "      <td>0.034613</td>\n",
       "      <td>0.044408</td>\n",
       "      <td>0.018855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018766</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.589513</td>\n",
       "      <td>0.762973</td>\n",
       "      <td>0.070061</td>\n",
       "      <td>0.251497</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.131946</td>\n",
       "      <td>0.031522</td>\n",
       "      <td>0.895796</td>\n",
       "      <td>0.024561</td>\n",
       "      <td>0.037360</td>\n",
       "      <td>0.014279</td>\n",
       "      <td>0.026550</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.010212</td>\n",
       "      <td>0.379003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021818</td>\n",
       "      <td>0.004215</td>\n",
       "      <td>0.008566</td>\n",
       "      <td>0.014388</td>\n",
       "      <td>0.004221</td>\n",
       "      <td>0.059172</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.014837</td>\n",
       "      <td>0.031481</td>\n",
       "      <td>0.020583</td>\n",
       "      <td>0.033382</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.006163</td>\n",
       "      <td>0.011785</td>\n",
       "      <td>0.013588</td>\n",
       "      <td>0.037102</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.172222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.414966</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.133127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.045491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.069452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.019417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073394</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.105392</td>\n",
       "      <td>0.228972</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096154</td>\n",
       "      <td>0.058699</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.024876</td>\n",
       "      <td>0.046154</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.397428</td>\n",
       "      <td>0.006021</td>\n",
       "      <td>0.019456</td>\n",
       "      <td>0.002194</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>1.996352e-08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.722397</td>\n",
       "      <td>0.001722</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.008594</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.007791</td>\n",
       "      <td>0.023729</td>\n",
       "      <td>0.401295</td>\n",
       "      <td>0.049170</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.011459</td>\n",
       "      <td>0.003568</td>\n",
       "      <td>0.007343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.152104</td>\n",
       "      <td>0.469160</td>\n",
       "      <td>1.867351e-07</td>\n",
       "      <td>0.998505</td>\n",
       "      <td>0.008063</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559229</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.025729</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.033457</td>\n",
       "      <td>0.002889</td>\n",
       "      <td>2.878925e-04</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.012311</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008144</td>\n",
       "      <td>0.023026</td>\n",
       "      <td>0.048822</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.137584</td>\n",
       "      <td>0.124835</td>\n",
       "      <td>0.022563</td>\n",
       "      <td>0.029940</td>\n",
       "      <td>0.025994</td>\n",
       "      <td>0.007715</td>\n",
       "      <td>0.018427</td>\n",
       "      <td>0.106223</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.007740</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>0.020917</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.006723</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>0.405742</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030909</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.006444</td>\n",
       "      <td>0.008993</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.074212</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.019288</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.032590</td>\n",
       "      <td>0.033501</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.004622</td>\n",
       "      <td>0.043771</td>\n",
       "      <td>0.001524</td>\n",
       "      <td>0.022674</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.014370</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.006138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.169655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003877</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.089783</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.048259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001718</td>\n",
       "      <td>0.055890</td>\n",
       "      <td>0.012379</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.009709</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.095588</td>\n",
       "      <td>0.074766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059845</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003483</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>0.010204</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038621</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.822818</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.003708</td>\n",
       "      <td>0.006149</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.006872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.401177</td>\n",
       "      <td>0.048939</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003506</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004585</td>\n",
       "      <td>0.093851</td>\n",
       "      <td>0.470798</td>\n",
       "      <td>1.078271e-05</td>\n",
       "      <td>0.998505</td>\n",
       "      <td>0.015702</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559102</td>\n",
       "      <td>0.229814</td>\n",
       "      <td>0.049743</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.033457</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005185</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>0.004183</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>9.457842e-03</td>\n",
       "      <td>0.008144</td>\n",
       "      <td>0.027961</td>\n",
       "      <td>0.011785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.135542</td>\n",
       "      <td>0.124410</td>\n",
       "      <td>0.024794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>0.004099</td>\n",
       "      <td>0.018399</td>\n",
       "      <td>0.105431</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>0.005509</td>\n",
       "      <td>0.020705</td>\n",
       "      <td>0.027820</td>\n",
       "      <td>0.006052</td>\n",
       "      <td>0.003390</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.003361</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>0.213819</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023636</td>\n",
       "      <td>0.005849</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.060921</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.007418</td>\n",
       "      <td>0.012963</td>\n",
       "      <td>0.012007</td>\n",
       "      <td>0.032930</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.011138</td>\n",
       "      <td>0.019435</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.016845</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>0.006109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.164590</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.030960</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.048873</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>0.004192</td>\n",
       "      <td>0.084666</td>\n",
       "      <td>0.011786</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.116505</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.119266</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.024510</td>\n",
       "      <td>0.065421</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.058847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321839</td>\n",
       "      <td>0.120446</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.039735</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005652</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.400884</td>\n",
       "      <td>0.055368</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002674</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.001594</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032362</td>\n",
       "      <td>0.468032</td>\n",
       "      <td>1.065276e-05</td>\n",
       "      <td>0.998414</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559074</td>\n",
       "      <td>0.602484</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.005576</td>\n",
       "      <td>0.002319</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.017284</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.013181</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.012281</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>8.490852e-05</td>\n",
       "      <td>0.012216</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>0.011785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.130708</td>\n",
       "      <td>0.125538</td>\n",
       "      <td>0.025123</td>\n",
       "      <td>0.029940</td>\n",
       "      <td>0.007645</td>\n",
       "      <td>0.027677</td>\n",
       "      <td>0.018436</td>\n",
       "      <td>0.106414</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>0.020158</td>\n",
       "      <td>0.005455</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.005997</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.010084</td>\n",
       "      <td>0.012171</td>\n",
       "      <td>0.348582</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003636</td>\n",
       "      <td>0.005969</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.005975</td>\n",
       "      <td>0.059172</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.007418</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.033058</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.004622</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.001567</td>\n",
       "      <td>0.012367</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.150939</td>\n",
       "      <td>0.800436</td>\n",
       "      <td>0.004798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.168704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.065015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.055704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004919</td>\n",
       "      <td>0.059183</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.097087</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.174312</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.058251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.937931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408289</td>\n",
       "      <td>0.001545</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111702</td>\n",
       "      <td>0.026490</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.001377</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008802</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.401868</td>\n",
       "      <td>0.058966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.002012</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067961</td>\n",
       "      <td>0.467945</td>\n",
       "      <td>2.313983e-05</td>\n",
       "      <td>0.998447</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.151786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560065</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.011083</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.009294</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>3.060034e-11</td>\n",
       "      <td>0.010370</td>\n",
       "      <td>0.008818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003235</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.134115</td>\n",
       "      <td>0.129185</td>\n",
       "      <td>0.028947</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.006116</td>\n",
       "      <td>0.021349</td>\n",
       "      <td>0.019711</td>\n",
       "      <td>0.112194</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>0.017456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003554</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.001681</td>\n",
       "      <td>0.013845</td>\n",
       "      <td>0.527512</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007273</td>\n",
       "      <td>0.009499</td>\n",
       "      <td>0.002069</td>\n",
       "      <td>0.003597</td>\n",
       "      <td>0.009504</td>\n",
       "      <td>0.059172</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.007418</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>0.005146</td>\n",
       "      <td>0.034168</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005398</td>\n",
       "      <td>0.008245</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.194535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312925</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.063035</td>\n",
       "      <td>0.055189</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.006270</td>\n",
       "      <td>0.083918</td>\n",
       "      <td>0.002881</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.184466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.678899</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.107843</td>\n",
       "      <td>0.144860</td>\n",
       "      <td>0.011171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.055873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.199005</td>\n",
       "      <td>0.061538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.777429</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>0.100836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026788</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.040265e-09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.05694</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.800210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.255319</td>\n",
       "      <td>0.337748</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.003251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.008645</td>\n",
       "      <td>0.542373</td>\n",
       "      <td>0.401045</td>\n",
       "      <td>0.076217</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.006177</td>\n",
       "      <td>0.003961</td>\n",
       "      <td>0.043562</td>\n",
       "      <td>0.003980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122977</td>\n",
       "      <td>0.463270</td>\n",
       "      <td>9.006084e-06</td>\n",
       "      <td>0.996446</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.205357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.576959</td>\n",
       "      <td>0.310559</td>\n",
       "      <td>0.017153</td>\n",
       "      <td>0.018854</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.016729</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>9.785662e-05</td>\n",
       "      <td>0.015556</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>0.005726</td>\n",
       "      <td>0.004127</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.030993</td>\n",
       "      <td>2.241312e-05</td>\n",
       "      <td>0.006108</td>\n",
       "      <td>0.037829</td>\n",
       "      <td>0.020623</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.005646</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151513</td>\n",
       "      <td>0.145106</td>\n",
       "      <td>0.074841</td>\n",
       "      <td>0.203593</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.004980</td>\n",
       "      <td>0.021105</td>\n",
       "      <td>0.121597</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>0.001911</td>\n",
       "      <td>0.006206</td>\n",
       "      <td>0.020209</td>\n",
       "      <td>0.002012</td>\n",
       "      <td>0.007199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015126</td>\n",
       "      <td>0.003861</td>\n",
       "      <td>0.503568</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021818</td>\n",
       "      <td>0.006544</td>\n",
       "      <td>0.004928</td>\n",
       "      <td>0.023381</td>\n",
       "      <td>0.006549</td>\n",
       "      <td>0.061561</td>\n",
       "      <td>0.001073</td>\n",
       "      <td>0.013353</td>\n",
       "      <td>0.024074</td>\n",
       "      <td>0.008576</td>\n",
       "      <td>0.071138</td>\n",
       "      <td>0.047202</td>\n",
       "      <td>0.006163</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.008923</td>\n",
       "      <td>0.049470</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.019444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          M1        M2        M3        M4        M5        M6        M7        M8        M9       M10       M11       M12    M13       M14       M15       M16    M17       M18       M19       M20       M21       M22       M23       M24       M25       M26       M27       M28       M29       M30       M31       M32  \\\n",
       "0   0.165297  0.000000  0.007310  0.136054       NaN  0.142415       NaN  0.044358  0.000000  0.000001  0.055411       NaN  0.160  0.009709  0.000000  0.000000  0.056  0.127451  0.126168  0.000000  0.000000  0.000000  0.059037  0.000000  0.020896  0.046154  0.040816  0.215172  0.000000  0.000177  0.000143  0.000006   \n",
       "1   0.164408  0.000875       NaN  0.088435       NaN  0.074303       NaN  0.044633  0.000000  0.000601  0.053411  0.001036  0.024  0.058252  0.090909  0.128440  0.016  0.022059  0.065421  0.001486  0.024096  0.134615  0.055487  0.010751       NaN  0.030769  0.020408  0.293546  0.000000       NaN  0.003410  0.000050   \n",
       "2   0.199866  0.000000  0.000000  0.095238       NaN  0.037152       NaN       NaN  0.000000  0.000011  0.053411  0.000000  0.192  0.000000  0.000000  0.000000  0.040  0.093137  0.074766  0.000001  0.024096  0.000000  0.097537  0.000000       NaN  0.000000  0.010204  0.501262  0.000000  0.002791  0.003465  0.000500   \n",
       "3   0.293155  0.000000  0.000000  0.095238       NaN  0.058824       NaN  0.059858  0.115385  0.013079  0.059307  0.008233  0.080  0.184466  0.000000  0.137615  0.160  0.061275  0.084112  0.007869  0.000000  0.067308  0.066598  0.004715  0.099502  0.138462  0.000000  0.047542  0.037905  0.164819  0.008220  0.000307   \n",
       "4   0.205843  0.000855  0.000000  0.095238       NaN  0.130031       NaN  0.080775  0.000000  0.154861  0.054944  0.125440  0.224  0.368932  0.090909  0.155963  0.336  0.078431  0.196262  0.044259  0.036145  0.240385  0.058851  0.000000  1.000000  0.061538  0.010204  0.425053  0.000722       NaN  0.002968  0.293959   \n",
       "..       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...    ...       ...       ...       ...    ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...   \n",
       "83  0.172222  0.000000  0.000255  0.414966  0.111111  0.133127       NaN  0.045491  0.000000  0.001475  0.069452  0.000000  0.688  0.019417  0.000000  0.073394  0.112  0.105392  0.228972  0.000606  0.000000  0.096154  0.058699  0.000430  0.024876  0.046154  0.040816  0.397428  0.006021  0.019456  0.002194  0.000086   \n",
       "84  0.169655  0.000000  0.003877  0.061224       NaN  0.089783       NaN  0.048259  0.000000  0.001718  0.055890  0.012379  0.072  0.009709  0.000000  0.000000  0.024  0.095588  0.074766  0.000000  0.024096  0.000000  0.059845  0.000000  0.003483  0.030769  0.010204  0.228571  0.000000  0.038621  0.000279  0.000025   \n",
       "85  0.164590  0.001197       NaN  0.000000       NaN  0.030960       NaN  0.048873  0.012821  0.004192  0.084666  0.011786  0.000  0.116505  0.136364  0.119266  0.016  0.024510  0.065421  0.001456  0.000000  0.009615  0.058847  0.000000       NaN  0.015385  0.000000  0.321839  0.120446       NaN  0.000300  0.001180   \n",
       "86  0.168704  0.000000  0.000000  0.020408       NaN  0.065015       NaN  0.055704  0.000000  0.004919  0.059183  0.000658  0.128  0.097087  0.181818  0.174312  0.016  0.000000  0.009346  0.002897  0.000000  0.019231  0.058251  0.000000       NaN  0.015385  0.000000  0.937931  0.000000  0.408289  0.001545  0.000004   \n",
       "87  0.194535  0.000000  0.000000  0.312925       NaN  0.105263  0.063035  0.055189  0.025641  0.006270  0.083918  0.002881  0.384  0.184466  0.000000  0.678899  0.032  0.107843  0.144860  0.011171  0.000000  0.423077  0.055873  0.000000  0.199005  0.061538  0.000000  0.777429  0.000637  0.100836  0.000000  0.026788   \n",
       "\n",
       "         M33           M34           M35      M36       M37       M38       M39       M40       M41       M42       M43       M44       M45       M46       M47       M48       M49       M50       M51       M52       M53       M54       M55       M56       M57       M58       M59       M60       M61           M62  \\\n",
       "0   0.083333           NaN  4.615261e-07      NaN  0.002829  0.000000       NaN  0.000299       NaN       NaN  0.066667  0.000000  0.005942  0.002306  0.045455  0.006796  0.111864  0.400547  0.048872  0.083333  0.545455  0.000000  0.012223  0.000191  0.001228  0.052632  0.004892  0.158576  0.462857  1.543597e-08   \n",
       "1        NaN  1.852666e-08  0.000000e+00      NaN  0.007072  0.000000       NaN       NaN  0.031915  0.211921  0.600000  0.533333  0.002969       NaN  0.015152  0.005667       NaN  0.401851  0.050445       NaN  0.363636  0.000000  0.006576  0.000138  0.000355  0.000000  0.000521  0.090615  0.467850  2.205454e-10   \n",
       "2   0.020833           NaN           NaN      NaN  0.000000  0.000000       NaN       NaN       NaN  0.026490  0.000000  0.266667  0.000839  0.000000  0.015152  0.049165       NaN  0.400910  0.048872  0.000000       NaN  0.005041  0.001347  0.006348  0.000499  0.052632  0.704746  0.038835  0.473481  1.302439e-06   \n",
       "3        NaN  7.150311e-09  4.984482e-05      NaN  0.041018  0.094595  0.800210       NaN  0.079787  0.119205  0.666667  0.600000  0.007984       NaN  0.106061  0.010376       NaN  0.402940  0.090888  0.083333  0.020202  0.025131  0.004219  0.006066  0.003171  0.052632  0.003264  0.058252  0.468859  2.760560e-06   \n",
       "4        NaN  6.176450e-08  0.000000e+00      NaN  0.015559  0.013514  0.800210       NaN  0.127660  0.456954  0.800000  0.466667  0.129841       NaN  0.000000  0.013508       NaN  0.400820  0.057965  0.166667  0.727273  0.000164  0.008081  0.000573  0.001468  0.000000  0.001019  0.174757  0.467919  9.596829e-08   \n",
       "..       ...           ...           ...      ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...           ...   \n",
       "83  0.041667  1.996352e-08  0.000000e+00      NaN  0.000000  0.013514  0.722397  0.001722  0.010638  0.013245  0.733333  0.400000  0.005059  0.008594  0.015152  0.007791  0.023729  0.401295  0.049170  0.750000  0.909091  0.000022  0.011459  0.003568  0.007343  0.000000  0.000488  0.152104  0.469160  1.867351e-07   \n",
       "84  0.041667           NaN  0.000000e+00      NaN  0.001414  0.000000  0.822818  0.001033       NaN  0.013245  0.466667  0.266667  0.003708  0.006149  0.030303  0.006872       NaN  0.401177  0.048939  0.166667  0.181818  0.000000  0.003506  0.003906  0.003872  0.000000  0.004585  0.093851  0.470798  1.078271e-05   \n",
       "85       NaN           NaN  0.000000e+00      NaN  0.001414  0.000000       NaN       NaN       NaN  0.039735  0.866667  0.600000  0.001302       NaN  0.000000  0.005652       NaN  0.400884  0.055368       NaN       NaN  0.000000  0.002674  0.000047  0.001594  0.157895  0.000000  0.032362  0.468032  1.065276e-05   \n",
       "86       NaN           NaN  0.000000e+00      NaN  0.001414  0.013514       NaN       NaN  0.111702  0.026490  0.866667  0.600000  0.001377       NaN  0.000000  0.008802       NaN  0.401868  0.058966  0.000000       NaN  0.000000  0.000198  0.000440  0.002012  0.052632  0.000000  0.067961  0.467945  2.313983e-05   \n",
       "87       NaN  6.040265e-09  0.000000e+00  0.05694  0.009901  0.013514  0.800210  0.000000  0.255319  0.337748  0.800000  0.600000  0.003251       NaN  0.030303  0.008645  0.542373  0.401045  0.076217  1.000000  0.363636  0.006177  0.003961  0.043562  0.003980  0.000000  0.000000  0.122977  0.463270  9.006084e-06   \n",
       "\n",
       "         M63       M64       M65  ...       M87       M88       M89       M90   M91       M92       M93       M94           M95       M96       M97       M98       M99      M100      M101          M102      M103      M104      M105      M106      M107      M108      M109      M110      M111      M112      M113  \\\n",
       "0        NaN  0.009931  0.062500  ...  0.559381  0.192547  0.054889  0.000000  0.00  0.066667  0.046468  0.000130           NaN       NaN       NaN  0.000996  0.000112  0.000000  0.000000           NaN       NaN  0.037829  0.021605  0.000000       NaN  0.005952  0.010101  0.131050  0.124341  0.022081       NaN   \n",
       "1   0.998366       NaN  0.035714  ...  0.559102  0.397516  0.008576  0.000643  0.04  0.533333  0.014870  0.003097  3.013948e-04  0.007778  0.000178  0.000000  0.000201  0.007018  0.000516  4.632732e-09  0.014660  0.019737  0.007856  0.036364  0.007475  0.017857  0.010101  0.130504  0.125236  0.023574  0.155689   \n",
       "2        NaN  0.021219  0.107143  ...  0.559451  0.366460  0.020583  0.008335  0.02  0.066667  0.027881  0.003147           NaN       NaN       NaN  0.003396  0.010646  0.001754  0.006906           NaN       NaN  0.011513  0.017677  0.000000  0.000732  0.005952       NaN  0.159576  0.124365  0.027273       NaN   \n",
       "3   0.979335       NaN  0.080357  ...  0.551525  0.105590  0.078902  0.050068  0.00  0.666667  0.078067  0.024010  4.143250e-04  0.096790  0.009735  0.020223  0.018867  0.098246  0.035452  1.828899e-04  0.099767  0.083882  0.082492  0.345455  0.102537  0.107143  0.106061  0.254165  0.259724  0.055989  0.125749   \n",
       "4   0.998416       NaN  0.160714  ...  0.559105  0.335404  0.015437  0.000807  0.04  0.800000  0.035316  0.003258  2.997761e-04  0.017284  0.014715  0.000000  0.015478  0.019298  0.000398  5.025978e-03  0.034613  0.044408  0.018855  0.000000  0.018766  0.023810  0.010101  0.589513  0.762973  0.070061  0.251497   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   ...       ...       ...       ...           ...       ...       ...       ...       ...       ...       ...           ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...   \n",
       "83  0.998505  0.008063  0.250000  ...  0.559229  0.142857  0.025729  0.000067  0.00  0.466667  0.033457  0.002889  2.878925e-04  0.007778  0.000114  0.012311  0.000957  0.003509  0.000111           NaN  0.008144  0.023026  0.048822  0.000000  0.000000  0.011905  0.010101  0.137584  0.124835  0.022563  0.029940   \n",
       "84  0.998505  0.015702  0.053571  ...  0.559102  0.229814  0.049743  0.000105  0.06  0.266667  0.033457  0.000254           NaN  0.005185  0.000149  0.001814  0.004183  0.003509  0.000094  9.457842e-03  0.008144  0.027961  0.011785  0.000000  0.000000       NaN  0.005051  0.135542  0.124410  0.024794  0.000000   \n",
       "85  0.998414       NaN  0.026786  ...  0.559074  0.602484  0.003431  0.001329  0.00  0.733333  0.005576  0.002319  1.000000e+00  0.017284  0.001603  0.013181  0.000613  0.012281  0.001705  8.490852e-05  0.012216  0.001645  0.011785  0.000000  0.001464  0.011905  0.010101  0.130708  0.125538  0.025123  0.029940   \n",
       "86  0.998447       NaN  0.151786  ...  0.560065  0.739130  0.003431  0.011083  0.00  0.466667  0.009294  0.001403  3.060034e-11  0.010370  0.008818  0.000000  0.003235  0.000000  0.003359           NaN  0.007330  0.000000  0.000000  0.018182  0.001464  0.005952  0.010101  0.134115  0.129185  0.028947  0.005988   \n",
       "87  0.996446       NaN  0.205357  ...  0.576959  0.310559  0.017153  0.018854  0.02  0.733333  0.016729  0.000024  9.785662e-05  0.015556  0.001137  0.005726  0.004127  0.010526  0.030993  2.241312e-05  0.006108  0.037829  0.020623  0.018182  0.005646  0.011905  0.000000  0.151513  0.145106  0.074841  0.203593   \n",
       "\n",
       "        M114      M115      M116      M117      M118      M119      M120      M121      M122      M123      M124      M125      M126      M127      M128      M129      M130      M131      M132      M133      M134      M135      M136      M137      M138      M139      M140      M141      M142      M143      M144  \\\n",
       "0   0.012232  0.003789  0.018387  0.105363  0.026316  0.003086  0.000406  0.048174  0.002995  0.001123  0.954545  0.000000  0.000372  0.195025  0.010417  0.000000  0.000000  0.030909  0.000000  0.001961  0.001799  0.000234  0.082142       NaN  0.022255  0.022222  0.068611  0.032930  0.000000  0.001541  0.020202   \n",
       "1   0.006116  0.000657  0.018941  0.109153  0.007018  0.000471  0.000131  0.015274  0.000347  0.000362  0.000000  0.011765  0.001312  0.229642  0.000000  0.015625  0.026667  0.018182  0.000116  0.002295  0.012590  0.000122  0.059172  0.000322  0.005935  0.016667  0.018868  0.032930  0.001858  0.007704  0.003367   \n",
       "2   0.004587  0.003604  0.018387  0.105608  0.003509  0.006987  0.003471  0.013983  0.003137  0.001567  1.000000  0.015126  0.000223  0.441284  0.000000  0.015625  0.000000  0.032727  0.001019  0.001961  0.000000  0.004198  0.061566       NaN  0.013353  0.018519  0.008576  0.041960  0.001092  0.000000  0.011785   \n",
       "3   0.085627  0.028155  0.036649  0.201047  0.077193  0.021059  0.019846  0.096169  0.018533  0.024603  0.045455  0.095798  0.019652  0.037149  0.114583  0.203125  0.093333  0.101818  0.010312  0.077384  0.082734  0.010318  0.067727  0.026758  0.078635  0.090741  0.089194  0.076674  0.019209  0.089368  0.074074   \n",
       "4   0.001529  0.131946  0.031522  0.895796  0.024561  0.037360  0.014279  0.026550  0.000203  0.001392  0.000000  0.016807  0.010212  0.379003  0.000000  0.000000  0.000000  0.021818  0.004215  0.008566  0.014388  0.004221  0.059172  0.000380  0.014837  0.031481  0.020583  0.033382  0.000288  0.006163  0.011785   \n",
       "..       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...   \n",
       "83  0.025994  0.007715  0.018427  0.106223  0.017544  0.007740  0.002128  0.020917  0.002899  0.000133  0.045455  0.006723  0.001955  0.405742  0.020833  0.000000  0.000000  0.030909  0.000565  0.006444  0.008993  0.000516  0.074212  0.000179  0.019288  0.027778  0.032590  0.033501  0.000093  0.004622  0.043771   \n",
       "84  0.009174  0.004099  0.018399  0.105431  0.015789  0.005509  0.020705  0.027820  0.006052  0.003390  0.454545  0.003361  0.001568  0.213819  0.010417  0.000000  0.000000  0.023636  0.005849  0.001307  0.001799  0.000039  0.060921  0.000010  0.007418  0.012963  0.012007  0.032930  0.000079  0.001541  0.010101   \n",
       "85  0.007645  0.027677  0.018436  0.106414  0.005263  0.020158  0.005455  0.003433  0.000075  0.005997  0.136364  0.010084  0.012171  0.348582  0.031250  0.000000  0.000000  0.003636  0.005969  0.000970  0.005396  0.005975  0.059172  0.000254  0.007418  0.007407  0.003431  0.033058  0.000981  0.004622  0.001684   \n",
       "86  0.006116  0.021349  0.019711  0.112194  0.005263  0.017456  0.000000  0.003554  0.000070  0.000000  0.045455  0.001681  0.013845  0.527512  0.010417  0.015625  0.000000  0.007273  0.009499  0.002069  0.003597  0.009504  0.059172  0.000218  0.007418  0.003704  0.005146  0.034168  0.000584  0.001541  0.000000   \n",
       "87  0.013761  0.004980  0.021105  0.121597  0.005263  0.001911  0.006206  0.020209  0.002012  0.007199  0.000000  0.015126  0.003861  0.503568  0.010417  0.015625  0.000000  0.021818  0.006544  0.004928  0.023381  0.006549  0.061561  0.001073  0.013353  0.024074  0.008576  0.071138  0.047202  0.006163  0.015152   \n",
       "\n",
       "        M145      M146      M147      M148      M149      M150      M151  \n",
       "0   0.001009  0.031802  0.066667  0.000030  0.010525  0.000115  0.000000  \n",
       "1   0.000399  0.018551  0.733333  0.000027  0.014329       NaN  0.006330  \n",
       "2   0.001946  0.006184  0.066667  0.000107  0.015978  0.000107  0.010795  \n",
       "3   0.022492  0.101001  0.666667  0.000022  0.033444  0.000139  0.005615  \n",
       "4   0.013588  0.037102  0.666667  0.000030       NaN       NaN  0.006396  \n",
       "..       ...       ...       ...       ...       ...       ...       ...  \n",
       "83  0.001524  0.022674  0.533333  0.000039  0.014370  0.000116  0.006138  \n",
       "84  0.011138  0.019435  0.266667  0.000046  0.016845  0.002738  0.006109  \n",
       "85  0.001567  0.012367  0.666667  0.000021  0.150939  0.800436  0.004798  \n",
       "86  0.005398  0.008245  0.466667  0.000201       NaN       NaN  0.007221  \n",
       "87  0.008923  0.049470  1.000000  0.000050  0.019444       NaN  0.001904  \n",
       "\n",
       "[88 rows x 151 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import module\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scale features\n",
    "scaler = MinMaxScaler()\n",
    "model=scaler.fit(df_test)\n",
    "scaled_df_test=model.transform(df_test)\n",
    "\n",
    "# creating a data frame to put into KNN imputer\n",
    "df_test=pd.DataFrame(scaled_df_test,columns=df_test.columns)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc63c6c",
   "metadata": {},
   "source": [
    "### IMPUTING NULL VALUES USING KNN IMPUTATION WITH K=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "041147c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using KNN imputer to impute the data\n",
    "from sklearn.impute import KNNImputer\n",
    "import pandas as pd\n",
    "\n",
    "# Create an instance of the KNNImputer class\n",
    "imputer = KNNImputer(n_neighbors=1)\n",
    "\n",
    "# Fit and transform the DataFrame to impute missing values\n",
    "df_test_imputed = imputer.fit_transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "21f1088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_imputed = pd.DataFrame(df_test_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7011e056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverse transform of Knn Imputed File\n",
    "df_test_imputed=pd.DataFrame(scaler.inverse_transform(df_test_imputed),columns=df_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "5ff8c2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0a701773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'M10',\n",
       "       ...\n",
       "       'M143', 'M144', 'M145', 'M146', 'M147', 'M148', 'M149', 'M150', 'M151', 'target'], dtype='object', length=152)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df1_test.target\n",
    "\n",
    "join= [df_test_imputed,target] \n",
    "df_test_imputed = pd.concat(join,axis=1,join='inner')  \n",
    "df_test_imputed.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca9c70d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7216ef4a",
   "metadata": {},
   "source": [
    "# MODEL FITTING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c636bfed",
   "metadata": {},
   "source": [
    "### BASE LINE MODEL - WITH IMBALANCED DATA\n",
    "**Fitted multiple classification models using k-fold cross-validation on the training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fe93b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train_imputed.drop([\"target\"],axis=1)  #predictors, dropping y column (Target)\n",
    "y_train = df_train_imputed[\"target\"]                # y as target column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b1d7ae",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dfff54b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score using 10-fold Cross Validation: 0.0\n"
     ]
    }
   ],
   "source": [
    "# create a logistic regression model\n",
    "lr = LogisticRegression(random_state=10)\n",
    "\n",
    "# fit the model on the training data\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Performance check using cross validation technique\n",
    "scores = cross_val_score(lr, X_train, y_train, cv=10, scoring='f1')\n",
    "\n",
    "#Print the f1 score\n",
    "print(\"F1 score using 10-fold Cross Validation:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630612f2",
   "metadata": {},
   "source": [
    "## DECISION TREE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "363f6eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score using 10-fold Cross Validation: 0.802021926331842\n"
     ]
    }
   ],
   "source": [
    "#Fitting Decision tree classifier Model\n",
    "dt = DecisionTreeClassifier(random_state=10)\n",
    "\n",
    "# Performance check using cross validation technique\n",
    "scores = cross_val_score(dt, X_train, y_train, cv=10, scoring='f1')\n",
    "\n",
    "#Print the f1 score\n",
    "print(\"F1 score using 10-fold Cross Validation:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e2f8cc",
   "metadata": {},
   "source": [
    "## RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39dbef42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score using 10-fold Cross Validation: 0.864827667632154\n"
     ]
    }
   ],
   "source": [
    "rand_forest = RandomForestClassifier(random_state=10)\n",
    "rand_forest.fit(X_train,y_train)\n",
    "\n",
    "# Performance check using cross validation technique\n",
    "scores = cross_val_score(rand_forest, X_train, y_train, cv=10, scoring='f1')\n",
    "\n",
    "#Print the f1 score\n",
    "print(\"F1 score using 10-fold Cross Validation:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4760e41",
   "metadata": {},
   "source": [
    "## ADABOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5715720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score using 10-fold Cross Validation: 0.8234120820204958\n"
     ]
    }
   ],
   "source": [
    "#Fitting adaboost tree classifier Model\n",
    "ada_boost = AdaBoostClassifier(random_state=10)\n",
    "ada_boost.fit(X_train, y_train)\n",
    "\n",
    "# Performance check using cross validation technique\n",
    "scores = cross_val_score(ada_boost, X_train, y_train, cv=10, scoring='f1')\n",
    "\n",
    "#Print the f1 score\n",
    "print(\"F1 score using 10-fold Cross Validation:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392fa196",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6afffd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score using 10-fold Cross Validation: 0.8615705539286764\n"
     ]
    }
   ],
   "source": [
    "#Fitting XGBOOST classifier Model\n",
    "xg = XGBClassifier(random_state=10)\n",
    "xg.fit(X_train,y_train)\n",
    "\n",
    "# Performance check using cross validation technique\n",
    "scores = cross_val_score(xg, X_train, y_train, cv=10, scoring='f1')\n",
    "\n",
    "#Print the f1 score\n",
    "print(\"F1 score using 10-fold Cross Validation:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971066e9",
   "metadata": {},
   "source": [
    "### MODEL -2 : WITH BALANCED DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a858d014",
   "metadata": {},
   "source": [
    "> **Balancing the imbalanced target class using SMOTE**\n",
    "    \n",
    "  **SMOTE generates synthetic examples of the minority class to balance the class distribution in imbalanced data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6bdbfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import SMOTE module from imblearn library\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(sampling_strategy=1,random_state = 10)\n",
    "X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c706688",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28526b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score using 10-fold Cross Validation: 0.0\n"
     ]
    }
   ],
   "source": [
    "# create a logistic regression model on balanced data\n",
    "lr = LogisticRegression(random_state=10)\n",
    "\n",
    "# fit the model on the training data\n",
    "lr.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "# Performance check using cross validation technique\n",
    "scores = cross_val_score(lr, X_train_sm, y_train_sm, cv=10, scoring='f1')\n",
    "\n",
    "#Print the f1 score\n",
    "print(\"F1 score using 10-fold Cross Validation:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83ce731",
   "metadata": {},
   "source": [
    "## DECISION TREE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cfc3a6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score using 10-fold Cross Validation: 0.7683669464038794\n"
     ]
    }
   ],
   "source": [
    "#Fitting Decision tree classifier Model on balanced data\n",
    "dt = DecisionTreeClassifier(random_state=10)\n",
    "\n",
    "# Performance check using cross validation technique\n",
    "scores = cross_val_score(dt, X_train_sm, y_train_sm, cv=10, scoring='f1')\n",
    "\n",
    "#Print the f1 score\n",
    "print(\"F1 score using 10-fold Cross Validation:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e82780d",
   "metadata": {},
   "source": [
    "## RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f353374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score using 10-fold Cross Validation: 0.854633115278974\n"
     ]
    }
   ],
   "source": [
    "#fitting random forest classifier model on balanced data\n",
    "rand_forest = RandomForestClassifier(random_state=10)\n",
    "\n",
    "# Performance check using cross validation technique\n",
    "scores = cross_val_score(rand_forest, X_train_sm, y_train_sm, cv=10, scoring='f1')\n",
    "\n",
    "#Print the f1 score\n",
    "print(\"F1 score using 10-fold Cross Validation:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c294bb5",
   "metadata": {},
   "source": [
    "## ADABOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0b9e8fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score using 10-fold Cross Validation: 0.8361580908164757\n"
     ]
    }
   ],
   "source": [
    "#Fitting adaboost tree classifier Model on balanced data\n",
    "ada_boost = AdaBoostClassifier(random_state=10)\n",
    "\n",
    "# Performance check using cross validation technique\n",
    "scores = cross_val_score(ada_boost, X_train_sm, y_train_sm, cv=10, scoring='f1')\n",
    "\n",
    "#Print the f1 score\n",
    "print(\"F1 score using 10-fold Cross Validation:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef75b7f0",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bdd329b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score using 10-fold Cross Validation: 0.8641711337106337\n"
     ]
    }
   ],
   "source": [
    "#Fitting XGBOOST classifier Model on balanced data\n",
    "xg = XGBClassifier(random_state=10)\n",
    "\n",
    "# Performance check using cross validation technique\n",
    "scores = cross_val_score(xg, X_train_sm, y_train_sm, cv=10, scoring='f1')\n",
    "\n",
    "#Print the f1 score\n",
    "print(\"F1 score using 10-fold Cross Validation:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d96016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e32a849",
   "metadata": {},
   "source": [
    "## Feature Selection: Information Value/Weight of Evidence Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7b25ea",
   "metadata": {},
   "source": [
    "**Information Value (IV) or Weight of Evidence (WoE) is used to evaluate the predictive power of a feature in a binary classification problem by measuring the strength of its association with the target variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ee17005",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install monotonic-binning\n",
    "#import monotonic binning to automatically calculate IV values for numeric values\n",
    "from monotonic_binning.monotonic_woe_binning import Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "34b343b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Side by Side boxplot\n",
    "numeric = df_train_imputed.select_dtypes(exclude=\"object\")\n",
    "numeric = numeric.drop([\"target\"],axis=1)\n",
    "# numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "efd5cacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin_object.woe_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "faddb28d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M1 :  0.0\n",
      "M2 :  0.0\n",
      "M3 :  0.0\n",
      "M4 :  0.0\n",
      "M5 :  0.05555369208183761\n",
      "M6 :  0.0\n",
      "M7 :  0.008161856555665627\n",
      "M8 :  0.0\n",
      "M9 :  0.0\n",
      "M10 :  0.0\n",
      "M11 :  0.0\n",
      "M12 :  0.005176777598298434\n",
      "M13 :  0.0\n",
      "M14 :  0.0\n",
      "M15 :  0.198591166165609\n",
      "M16 :  0.0\n",
      "M17 :  0.0\n",
      "M18 :  0.0\n",
      "M19 :  0.0\n",
      "M20 :  0.0\n",
      "M21 :  0.0\n",
      "M22 :  0.0\n",
      "M23 :  0.0\n",
      "M24 :  0.0\n",
      "M25 :  0.1358197042673053\n",
      "M26 :  0.0\n",
      "M27 :  0.0\n",
      "M28 :  0.3326202083411199\n",
      "M29 :  0.0\n",
      "M30 :  0.0\n",
      "M31 :  0.0\n",
      "M32 :  0.0\n",
      "M33 :  0.0\n",
      "M34 :  0.019896293382308184\n",
      "M35 :  0.0\n",
      "M36 :  0.0\n",
      "M37 :  0.0\n",
      "M38 :  0.0\n",
      "M39 :  0.0\n",
      "M40 :  0.0\n",
      "M41 :  0.0\n",
      "M42 :  0.0\n",
      "M43 :  0.0\n",
      "M44 :  0.0\n",
      "M45 :  0.0\n",
      "M46 :  0.0\n",
      "M47 :  0.0\n",
      "M48 :  0.0\n",
      "M49 :  0.0\n",
      "M50 :  0.0\n",
      "M51 :  0.0\n",
      "M52 :  0.0\n",
      "M53 :  0.16427143831202237\n",
      "M54 :  0.0\n",
      "M55 :  0.0\n",
      "M56 :  0.0\n",
      "M57 :  0.0\n",
      "M58 :  0.05674508903695746\n",
      "M59 :  0.0\n",
      "M60 :  0.0\n",
      "M61 :  0.0\n",
      "M62 :  0.0\n",
      "M63 :  0.5115827291459275\n",
      "M64 :  0.0\n",
      "M65 :  0.0\n",
      "M66 :  0.32165827407538605\n",
      "M67 :  0.0\n",
      "M68 :  0.0\n",
      "M69 :  0.18546617153585138\n",
      "M70 :  0.0\n",
      "M71 :  0.0\n",
      "M72 :  0.025550278501137592\n",
      "M73 :  0.0\n",
      "M74 :  0.0\n",
      "M75 :  0.0\n",
      "M76 :  0.0\n",
      "M77 :  0.0\n",
      "M78 :  0.0\n",
      "M79 :  0.0\n",
      "M80 :  0.0\n",
      "M81 :  0.0\n",
      "M82 :  0.0\n",
      "M83 :  0.0\n",
      "M84 :  0.0\n",
      "M85 :  0.0\n",
      "M86 :  0.0\n",
      "M87 :  0.5819951292379695\n",
      "M88 :  0.4307595929690437\n",
      "M89 :  0.0\n",
      "M90 :  0.0\n",
      "M91 :  0.0\n",
      "M92 :  0.0\n",
      "M93 :  0.0\n",
      "M94 :  0.0\n",
      "M95 :  0.0\n",
      "M96 :  0.0\n",
      "M97 :  0.0\n",
      "M98 :  0.0\n",
      "M99 :  0.0\n",
      "M100 :  0.0\n",
      "M101 :  0.0\n",
      "M102 :  0.0\n",
      "M103 :  0.0\n",
      "M104 :  0.0\n",
      "M105 :  0.0\n",
      "M106 :  0.0\n",
      "M107 :  0.0\n",
      "M108 :  0.0\n",
      "M109 :  0.0\n",
      "M110 :  0.0\n",
      "M111 :  0.0\n",
      "M112 :  0.0\n",
      "M113 :  0.0\n",
      "M114 :  0.0\n",
      "M115 :  0.0\n",
      "M116 :  0.0\n",
      "M117 :  0.0\n",
      "M118 :  0.0\n",
      "M119 :  0.0\n",
      "M120 :  0.0\n",
      "M121 :  0.0\n",
      "M122 :  0.0\n",
      "M123 :  0.0\n",
      "M124 :  0.3698004027922841\n",
      "M125 :  0.0\n",
      "M126 :  0.0\n",
      "M127 :  0.218622282134406\n",
      "M128 :  0.0\n",
      "M129 :  0.0\n",
      "M130 :  0.0\n",
      "M131 :  0.0\n",
      "M132 :  0.0\n",
      "M133 :  0.0\n",
      "M134 :  0.0\n",
      "M135 :  0.0\n",
      "M136 :  0.0\n",
      "M137 :  0.0\n",
      "M138 :  0.0\n",
      "M139 :  0.0\n",
      "M140 :  0.0\n",
      "M141 :  0.0\n",
      "M142 :  0.0\n",
      "M143 :  0.0\n",
      "M144 :  0.0\n",
      "M145 :  0.0\n",
      "M146 :  0.0\n",
      "M147 :  0.0\n",
      "M148 :  0.44061581899154234\n",
      "M149 :  0.02151898161106464\n",
      "M150 :  0.061136245818932164\n",
      "M151 :  0.06960955628509816\n"
     ]
    }
   ],
   "source": [
    "woe={}\n",
    "iv={}          \n",
    "for i in numeric:\n",
    "    var = i     \n",
    "    y_var = \"target\" \n",
    "\n",
    "    bin_object = Binning(y_var, n_threshold = 50, y_threshold = 10, p_threshold = 0.35, sign=False)\n",
    "    bin_object.fit(df_train_imputed[[y_var, var]])\n",
    "\n",
    "    # Print WOE summary\n",
    "    print(i,\": \",np.sum(bin_object.woe_summary['IV_components']))\n",
    "    iv[i]=(np.sum(bin_object.woe_summary['IV_components']))\n",
    "    woe[f'woe_{i}'] = bin_object.woe_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677b5a02",
   "metadata": {},
   "source": [
    ">**Features with an IV value above the threshold of 0.01 were selected for analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b63dd66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_woe = df_train_imputed[['M5','M15','M25','M28','M34','M53','M58','M63','M66','M69','M72','M87','M88','M124','M127','M148','M149','M150','M151']]\n",
    "y_train_woe = df_train_imputed[\"target\"]   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eb54c0",
   "metadata": {},
   "source": [
    ">**The set of features selected from the training data based on the IV threshold of 0.01 was also used to filter the features in the test data to ensure consistency in the feature space between the two datasets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "724208b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test_imputed[['M5','M15','M25','M28','M34','M53','M58','M63','M66','M69','M72','M87','M88','M124','M127','M148','M149','M150','M151']]\n",
    "#predictors, dropping y column (Target)\n",
    "y_test = df_test_imputed[\"target\"]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe8bf5a",
   "metadata": {},
   "source": [
    "## MODEL FITTING -IMBALANCED DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1cbce7",
   "metadata": {},
   "source": [
    "#### Based on the results of k-fold cross validation, where the selected features were used to fit various models including Random Forest Classifier, AdaBoost, and XGBoost, and achieved good F1 scores, these models were selected to validate the test data.\n",
    "\n",
    ">**selected features were used to fit the models for the analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7c8f70",
   "metadata": {},
   "source": [
    "## RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "63693790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.86\n",
      "Recall: 0.48\n",
      "F1 score: 0.62\n",
      "AUROC: 0.72\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.48      0.62        25\n",
      "           1       0.82      0.97      0.89        63\n",
      "\n",
      "    accuracy                           0.83        88\n",
      "   macro avg       0.84      0.72      0.75        88\n",
      "weighted avg       0.83      0.83      0.81        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rand_forest = RandomForestClassifier()\n",
    "rand_forest.fit(X_train_woe,y_train_woe)\n",
    "y_pred_rf = rand_forest.predict(X_test)\n",
    "\n",
    "# calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_test, y_pred_rf,pos_label=0)\n",
    "recall = recall_score(y_test, y_pred_rf,pos_label=0)\n",
    "f1 = f1_score(y_test, y_pred_rf,pos_label=0)\n",
    "\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1 score: {:.2f}\".format(f1))\n",
    "\n",
    "# calculate AUROC\n",
    "auroc = roc_auc_score(y_test, y_pred_rf)\n",
    "print(\"AUROC: {:.2f}\".format(auroc))\n",
    "\n",
    "#printing classification report\n",
    "print(classification_report(y_test,y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dc4675",
   "metadata": {},
   "source": [
    "## ADABOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "db8bccac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.78\n",
      "Recall: 0.56\n",
      "F1 score: 0.65\n",
      "AUROC: 0.75\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.56      0.65        25\n",
      "           1       0.84      0.94      0.89        63\n",
      "\n",
      "    accuracy                           0.83        88\n",
      "   macro avg       0.81      0.75      0.77        88\n",
      "weighted avg       0.82      0.83      0.82        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fitting adaboost tree classifier Model\n",
    "ada_boost = AdaBoostClassifier()\n",
    "ada_boost.fit(X_train_woe,y_train_woe)\n",
    "\n",
    "# make predictions on the testing data\n",
    "y_pred_ada = ada_boost.predict(X_test)\n",
    "\n",
    "# calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_test, y_pred_ada,pos_label=0)\n",
    "recall = recall_score(y_test, y_pred_ada,pos_label=0)\n",
    "f1 = f1_score(y_test, y_pred_ada,pos_label=0)\n",
    "\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1 score: {:.2f}\".format(f1))\n",
    "\n",
    "# calculate AUROC\n",
    "auroc = roc_auc_score(y_test, y_pred_ada)\n",
    "print(\"AUROC: {:.2f}\".format(auroc))\n",
    "\n",
    "#printing classification report\n",
    "print(classification_report(y_test,y_pred_ada))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9f8de",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b179baae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.67\n",
      "Recall: 0.48\n",
      "F1 score: 0.56\n",
      "AUROC: 0.69\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.48      0.56        25\n",
      "           1       0.81      0.90      0.86        63\n",
      "\n",
      "    accuracy                           0.78        88\n",
      "   macro avg       0.74      0.69      0.71        88\n",
      "weighted avg       0.77      0.78      0.77        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fitting XGBOOST classifier Model\n",
    "xg = XGBClassifier()\n",
    "xg.fit(X_train_woe,y_train_woe)\n",
    "\n",
    "# make predictions on the testing data\n",
    "y_pred_xg = xg.predict(X_test)\n",
    "\n",
    "# calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_test, y_pred_xg,pos_label=0)\n",
    "recall = recall_score(y_test, y_pred_xg,pos_label=0)\n",
    "f1 = f1_score(y_test, y_pred_xg,pos_label=0)\n",
    "\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1 score: {:.2f}\".format(f1))\n",
    "\n",
    "# calculate AUROC\n",
    "auroc = roc_auc_score(y_test, y_pred_xg)\n",
    "print(\"AUROC: {:.2f}\".format(auroc))\n",
    "\n",
    "#printing classification report\n",
    "print(classification_report(y_test,y_pred_xg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67db04b2",
   "metadata": {},
   "source": [
    "## Model - 4 : Balanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "02794b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import SMOTE module from imblearn library\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(sampling_strategy=1,random_state = 42)\n",
    "X_train_sm, y_train_sm = sm.fit_resample(X_train_woe, y_train_woe.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6396e86f",
   "metadata": {},
   "source": [
    "## RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c38aaf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.70\n",
      "Recall: 0.64\n",
      "F1 score: 0.67\n",
      "AUROC: 0.76\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.64      0.67        25\n",
      "           1       0.86      0.89      0.88        63\n",
      "\n",
      "    accuracy                           0.82        88\n",
      "   macro avg       0.78      0.76      0.77        88\n",
      "weighted avg       0.81      0.82      0.82        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rand_forest = RandomForestClassifier()\n",
    "rand_forest.fit(X_train_sm, y_train_sm)\n",
    "y_pred_rf = rand_forest.predict(X_test)\n",
    "\n",
    "# calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_test, y_pred_rf,pos_label=0)\n",
    "recall = recall_score(y_test, y_pred_rf,pos_label=0)\n",
    "f1 = f1_score(y_test, y_pred_rf,pos_label=0)\n",
    "\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1 score: {:.2f}\".format(f1))\n",
    "\n",
    "# calculate AUROC\n",
    "auroc = roc_auc_score(y_test, y_pred_rf)\n",
    "print(\"AUROC: {:.2f}\".format(auroc))\n",
    "\n",
    "#printing classification report\n",
    "print(classification_report(y_test,y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f44c691",
   "metadata": {},
   "source": [
    "## ADABOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5d0a1cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.66\n",
      "Recall: 0.76\n",
      "F1 score: 0.70\n",
      "AUROC: 0.80\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.70        25\n",
      "           1       0.90      0.84      0.87        63\n",
      "\n",
      "    accuracy                           0.82        88\n",
      "   macro avg       0.78      0.80      0.79        88\n",
      "weighted avg       0.83      0.82      0.82        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fitting adaboost tree classifier Model\n",
    "ada_boost = AdaBoostClassifier()\n",
    "ada_boost.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "# make predictions on the testing data\n",
    "y_pred_ada = ada_boost.predict(X_test)\n",
    "\n",
    "# calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_test, y_pred_ada,pos_label=0)\n",
    "recall = recall_score(y_test, y_pred_ada,pos_label=0)\n",
    "f1 = f1_score(y_test, y_pred_ada,pos_label=0)\n",
    "\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1 score: {:.2f}\".format(f1))\n",
    "\n",
    "# calculate AUROC\n",
    "auroc = roc_auc_score(y_test, y_pred_ada)\n",
    "print(\"AUROC: {:.2f}\".format(auroc))\n",
    "\n",
    "#printing classification report\n",
    "print(classification_report(y_test,y_pred_ada))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a729c42",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "915a84b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.62\n",
      "Recall: 0.64\n",
      "F1 score: 0.63\n",
      "AUROC: 0.74\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.64      0.63        25\n",
      "           1       0.85      0.84      0.85        63\n",
      "\n",
      "    accuracy                           0.78        88\n",
      "   macro avg       0.74      0.74      0.74        88\n",
      "weighted avg       0.79      0.78      0.79        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fitting XGBOOST classifier Model\n",
    "xg = XGBClassifier()\n",
    "xg.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "# make predictions on the testing data\n",
    "y_pred_xg = xg.predict(X_test)\n",
    "\n",
    "# calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_test, y_pred_xg,pos_label=0)\n",
    "recall = recall_score(y_test, y_pred_xg,pos_label=0)\n",
    "f1 = f1_score(y_test, y_pred_xg,pos_label=0)\n",
    "\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1 score: {:.2f}\".format(f1))\n",
    "\n",
    "# calculate AUROC\n",
    "auroc = roc_auc_score(y_test, y_pred_xg)\n",
    "print(\"AUROC: {:.2f}\".format(auroc))\n",
    "\n",
    "#printing classification report\n",
    "print(classification_report(y_test,y_pred_xg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e881e36c",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning of random forest classifier\n",
    ">I did hyperparameter tuning using GridSearchCV which is popular hyperparameter tuning technique used for optimizing machine learning models as it exhaustively searches over a pre-defined hyperparameter grid, making it more comprehensive than other tuning methods such as random search or manual tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5905d436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "# # define the hyperparameter space\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 300, 500],\n",
    "    \"max_depth\": [2,4,6,8,10,12,14,16],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    " }\n",
    "\n",
    "# # instantiate the Random Forest Classifier\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# # define the GridSearchCV object with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    rfc, \n",
    "    param_grid=param_grid, \n",
    "    cv=5\n",
    " )\n",
    "\n",
    "# # fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "# # print the best hyperparameters found\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f49c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Best Hyperparameters: {'max_depth': 16, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "31f9f90a",
   "metadata": {},
   "source": [
    "Best Hyperparameters: {'bootstrap': False, 'criterion': 'gini', 'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e93369",
   "metadata": {},
   "source": [
    "#### Fitted Random foresr model after hyperparameter tuning using Gridsearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "38c5ed5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.68\n",
      "Recall: 0.60\n",
      "F1 score: 0.64\n",
      "AUROC: 0.74\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.60      0.64        25\n",
      "           1       0.85      0.89      0.87        63\n",
      "\n",
      "    accuracy                           0.81        88\n",
      "   macro avg       0.77      0.74      0.75        88\n",
      "weighted avg       0.80      0.81      0.80        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rand_forest = RandomForestClassifier(max_depth = 10, min_samples_leaf= 1, min_samples_split= 2, n_estimators= 300)\n",
    "rand_forest.fit(X_train_sm, y_train_sm)\n",
    "y_pred_rf = rand_forest.predict(X_test)\n",
    "\n",
    "# calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_test, y_pred_rf,pos_label=0)\n",
    "recall = recall_score(y_test, y_pred_rf,pos_label=0)\n",
    "f1 = f1_score(y_test, y_pred_rf,pos_label=0)\n",
    "\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1 score: {:.2f}\".format(f1))\n",
    "\n",
    "# calculate AUROC\n",
    "auroc = roc_auc_score(y_test, y_pred_rf)\n",
    "print(\"AUROC: {:.2f}\".format(auroc))\n",
    "\n",
    "#printing classification report\n",
    "print(classification_report(y_test,y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcf4fb4",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning of Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1b6263f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.8653061224489796\n",
      "Best parameters: {'max_depth': 5, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Define XGBClassifier model\n",
    "model = XGBClassifier()\n",
    "\n",
    "# Perform GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "# Print the best hyperparameters and corresponding score\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "583d363f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.64\n",
      "Recall: 0.64\n",
      "F1 score: 0.64\n",
      "AUROC: 0.75\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.64      0.64        25\n",
      "           1       0.86      0.86      0.86        63\n",
      "\n",
      "    accuracy                           0.80        88\n",
      "   macro avg       0.75      0.75      0.75        88\n",
      "weighted avg       0.80      0.80      0.80        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fitting XGBOOST classifier Model\n",
    "xg = XGBClassifier(max_depth=5,n_estimators=100)\n",
    "xg.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "# make predictions on the testing data\n",
    "y_pred_xg = xg.predict(X_test)\n",
    "\n",
    "# calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_test, y_pred_xg,pos_label=0)\n",
    "recall = recall_score(y_test, y_pred_xg,pos_label=0)\n",
    "f1 = f1_score(y_test, y_pred_xg,pos_label=0)\n",
    "\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1 score: {:.2f}\".format(f1))\n",
    "\n",
    "# calculate AUROC\n",
    "auroc = roc_auc_score(y_test, y_pred_xg)\n",
    "print(\"AUROC: {:.2f}\".format(auroc))\n",
    "\n",
    "#printing classification report\n",
    "print(classification_report(y_test,y_pred_xg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6c249c",
   "metadata": {},
   "source": [
    "## Feature Reduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657b8b30",
   "metadata": {},
   "source": [
    "# VIF\n",
    ">Variance Inflation Factor (VIF) is used to identify and remove highly correlated features from a dataset to prevent multicollinearity in a regression model, and this process helps to improve the stability and interpretability of the model.\n",
    "\n",
    ">Although VIF is primarily used for regression models, it can also be used in classification problems to identify and remove highly correlated features that may affect the performance and interpretability of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4bcf07",
   "metadata": {},
   "source": [
    "**Did VIF on IV(information value) features to check which features are highly correlated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4518b61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature        VIF\n",
      "0       M5   2.258234\n",
      "1      M15   9.495633\n",
      "2      M25   1.779247\n",
      "3      M28  13.634026\n",
      "4      M34   1.094080\n",
      "5      M53   1.595883\n",
      "6      M58   8.956215\n",
      "7      M63   1.378619\n",
      "8      M66   1.295260\n",
      "9      M69  11.686293\n",
      "10     M72   1.279651\n",
      "11     M87   1.264775\n",
      "12     M88   8.940617\n",
      "13    M124   2.033934\n",
      "14    M127  13.236274\n",
      "15    M148   1.027130\n",
      "16    M149   1.033049\n",
      "17    M150   1.050956\n",
      "18    M151   1.174557\n"
     ]
    }
   ],
   "source": [
    "# VIF dataframe\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_train_woe.columns\n",
    "  \n",
    "# calculating VIF for each feature\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_train_woe.values, i)\n",
    "                          for i in range(len(X_train_woe.columns))]\n",
    "  \n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a704db",
   "metadata": {},
   "source": [
    "**>Features with a VIF score greater than 5 were identified and dropped from the dataset**\n",
    "\n",
    "\n",
    "**>As M28 has high VIF score i am dropping that feature from the data frame and we repeat the same process of dropping the feature one by one until we get the features with VIF score less than 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "19adda76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_woe_vif = X_train_woe.drop(['M28'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3717a7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature        VIF\n",
      "0       M5   2.254635\n",
      "1      M15   9.432948\n",
      "2      M25   1.752344\n",
      "3      M34   1.090109\n",
      "4      M53   1.475731\n",
      "5      M58   8.803379\n",
      "6      M63   1.378256\n",
      "7      M66   1.284265\n",
      "8      M69  11.674109\n",
      "9      M72   1.278480\n",
      "10     M87   1.263828\n",
      "11     M88   8.925818\n",
      "12    M124   2.031727\n",
      "13    M127   3.009498\n",
      "14    M148   1.024896\n",
      "15    M149   1.032471\n",
      "16    M150   1.050788\n",
      "17    M151   1.171779\n"
     ]
    }
   ],
   "source": [
    "# VIF dataframe\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_train_woe_vif.columns\n",
    "  \n",
    "# calculating VIF for each feature\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_train_woe_vif.values, i)\n",
    "                          for i in range(len(X_train_woe_vif.columns))]\n",
    "  \n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c7c3e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_woe_vif = X_train_woe_vif.drop(['M69'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "aa2e3e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature       VIF\n",
      "0       M5  2.250666\n",
      "1      M15  8.594785\n",
      "2      M25  1.746157\n",
      "3      M34  1.090087\n",
      "4      M53  1.475549\n",
      "5      M58  6.379379\n",
      "6      M63  1.368195\n",
      "7      M66  1.284221\n",
      "8      M72  1.268819\n",
      "9      M87  1.260584\n",
      "10     M88  8.033298\n",
      "11    M124  2.029126\n",
      "12    M127  3.002144\n",
      "13    M148  1.024825\n",
      "14    M149  1.029119\n",
      "15    M150  1.046209\n",
      "16    M151  1.170552\n"
     ]
    }
   ],
   "source": [
    "# VIF dataframe\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_train_woe_vif.columns\n",
    "  \n",
    "# calculating VIF for each feature\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_train_woe_vif.values, i)\n",
    "                          for i in range(len(X_train_woe_vif.columns))]\n",
    "  \n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "57786e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_woe_vif = X_train_woe_vif.drop(['M15'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "86a56d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature       VIF\n",
      "0       M5  2.137014\n",
      "1      M25  1.703379\n",
      "2      M34  1.020571\n",
      "3      M53  1.459818\n",
      "4      M58  2.933274\n",
      "5      M63  1.362859\n",
      "6      M66  1.282436\n",
      "7      M72  1.207155\n",
      "8      M87  1.259010\n",
      "9      M88  6.005323\n",
      "10    M124  1.968855\n",
      "11    M127  2.971247\n",
      "12    M148  1.021988\n",
      "13    M149  1.029032\n",
      "14    M150  1.043545\n",
      "15    M151  1.167655\n"
     ]
    }
   ],
   "source": [
    "# VIF dataframe\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_train_woe_vif.columns\n",
    "  \n",
    "# calculating VIF for each feature\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_train_woe_vif.values, i)\n",
    "                          for i in range(len(X_train_woe_vif.columns))]\n",
    "  \n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "334dcdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_woe_vif = X_train_woe_vif.drop(['M88'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ad4a5929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature       VIF\n",
      "0       M5  1.871470\n",
      "1      M25  1.633753\n",
      "2      M34  1.013651\n",
      "3      M53  1.458346\n",
      "4      M58  1.761253\n",
      "5      M63  1.356771\n",
      "6      M66  1.261411\n",
      "7      M72  1.161734\n",
      "8      M87  1.243576\n",
      "9     M124  1.913926\n",
      "10    M127  2.356671\n",
      "11    M148  1.015510\n",
      "12    M149  1.026811\n",
      "13    M150  1.043542\n",
      "14    M151  1.157417\n"
     ]
    }
   ],
   "source": [
    "# VIF dataframe\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_train_woe_vif.columns\n",
    "  \n",
    "# calculating VIF for each feature\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_train_woe_vif.values, i)\n",
    "                          for i in range(len(X_train_woe_vif.columns))]\n",
    "  \n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf1bb47",
   "metadata": {},
   "source": [
    "**And we are dropping those features in test data also**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3e26dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vif = X_test.drop(['M15','M28','M69','M88'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "47418ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['M5', 'M25', 'M34', 'M53', 'M58', 'M63', 'M66', 'M72', 'M87', 'M124', 'M127', 'M148', 'M149', 'M150', 'M151'], dtype='object')"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vif.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1602e344",
   "metadata": {},
   "source": [
    "## BALANCING THE DATA AFTER VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "bb911f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import SMOTE module from imblearn library\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(sampling_strategy=1,random_state = 42)\n",
    "X_train_sm_vif, y_train_sm_vif = sm.fit_resample(X_train_woe_vif, y_train_woe.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cdf567",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "62a91579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.60\n",
      "Recall: 0.60\n",
      "F1 score: 0.60\n",
      "AUROC: 0.72\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.60      0.60        25\n",
      "           1       0.84      0.84      0.84        63\n",
      "\n",
      "    accuracy                           0.77        88\n",
      "   macro avg       0.72      0.72      0.72        88\n",
      "weighted avg       0.77      0.77      0.77        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_vif = RandomForestClassifier()\n",
    "lr_vif.fit(X_train_sm_vif, y_train_sm_vif)\n",
    "y_pred_rf = lr_vif.predict(X_test_vif)\n",
    "\n",
    "# calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_test, y_pred_rf,pos_label=0)\n",
    "recall = recall_score(y_test, y_pred_rf,pos_label=0)\n",
    "f1 = f1_score(y_test, y_pred_rf,pos_label=0)\n",
    "\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1 score: {:.2f}\".format(f1))\n",
    "\n",
    "# calculate AUROC\n",
    "auroc = roc_auc_score(y_test, y_pred_rf)\n",
    "print(\"AUROC: {:.2f}\".format(auroc))\n",
    "\n",
    "#printing classification report\n",
    "print(classification_report(y_test,y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241a3bbc",
   "metadata": {},
   "source": [
    "## RANDOM FOREST ON BALACED DATA AFTER DOING VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "170ca691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.62\n",
      "Recall: 0.64\n",
      "F1 score: 0.63\n",
      "AUROC: 0.74\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.64      0.63        25\n",
      "           1       0.85      0.84      0.85        63\n",
      "\n",
      "    accuracy                           0.78        88\n",
      "   macro avg       0.74      0.74      0.74        88\n",
      "weighted avg       0.79      0.78      0.79        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rand_forest_vif = RandomForestClassifier()\n",
    "rand_forest_vif.fit(X_train_sm_vif, y_train_sm_vif)\n",
    "y_pred_rf = rand_forest_vif.predict(X_test_vif)\n",
    "\n",
    "# calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_test, y_pred_rf,pos_label=0)\n",
    "recall = recall_score(y_test, y_pred_rf,pos_label=0)\n",
    "f1 = f1_score(y_test, y_pred_rf,pos_label=0)\n",
    "\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1 score: {:.2f}\".format(f1))\n",
    "\n",
    "# calculate AUROC\n",
    "auroc = roc_auc_score(y_test, y_pred_rf)\n",
    "print(\"AUROC: {:.2f}\".format(auroc))\n",
    "\n",
    "#printing classification report\n",
    "print(classification_report(y_test,y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edf5a3d",
   "metadata": {},
   "source": [
    "## ADABOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "775ca195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.61\n",
      "Recall: 0.76\n",
      "F1 score: 0.68\n",
      "AUROC: 0.78\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.68        25\n",
      "           1       0.89      0.81      0.85        63\n",
      "\n",
      "    accuracy                           0.80        88\n",
      "   macro avg       0.75      0.78      0.76        88\n",
      "weighted avg       0.81      0.80      0.80        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fitting adaboost tree classifier Model\n",
    "ada_boost_vif = AdaBoostClassifier()\n",
    "ada_boost_vif.fit(X_train_sm_vif, y_train_sm)\n",
    "\n",
    "# make predictions on the testing data\n",
    "y_pred_ada = ada_boost_vif.predict(X_test_vif)\n",
    "\n",
    "# calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_test, y_pred_ada,pos_label=0)\n",
    "recall = recall_score(y_test, y_pred_ada,pos_label=0)\n",
    "f1 = f1_score(y_test, y_pred_ada,pos_label=0)\n",
    "\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1 score: {:.2f}\".format(f1))\n",
    "\n",
    "# calculate AUROC\n",
    "auroc = roc_auc_score(y_test, y_pred_ada)\n",
    "print(\"AUROC: {:.2f}\".format(auroc))\n",
    "\n",
    "#printing classification report\n",
    "print(classification_report(y_test,y_pred_ada))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027882ad",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e8b4609d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.58\n",
      "Recall: 0.60\n",
      "F1 score: 0.59\n",
      "AUROC: 0.71\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.60      0.59        25\n",
      "           1       0.84      0.83      0.83        63\n",
      "\n",
      "    accuracy                           0.76        88\n",
      "   macro avg       0.71      0.71      0.71        88\n",
      "weighted avg       0.76      0.76      0.76        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fitting XGBOOST classifier Model\n",
    "xg_vif = XGBClassifier()\n",
    "xg_vif.fit(X_train_sm_vif, y_train_sm_vif)\n",
    "\n",
    "# make predictions on the testing data\n",
    "y_pred_xg = xg_vif.predict(X_test_vif)\n",
    "\n",
    "# calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_test, y_pred_xg,pos_label=0)\n",
    "recall = recall_score(y_test, y_pred_xg,pos_label=0)\n",
    "f1 = f1_score(y_test, y_pred_xg,pos_label=0)\n",
    "\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1 score: {:.2f}\".format(f1))\n",
    "\n",
    "# calculate AUROC\n",
    "auroc = roc_auc_score(y_test, y_pred_xg)\n",
    "print(\"AUROC: {:.2f}\".format(auroc))\n",
    "\n",
    "#printing classification report\n",
    "print(classification_report(y_test,y_pred_xg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3136f3",
   "metadata": {},
   "source": [
    "## LASSO/RIDGE REGULARIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6af29d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge accuracy: 0.6818181818181818\n",
      "Precision: 0.43\n",
      "Recall: 0.40\n",
      "F1 score: 0.42\n",
      "AUROC: 0.60\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.40      0.42        25\n",
      "           1       0.77      0.79      0.78        63\n",
      "\n",
      "    accuracy                           0.68        88\n",
      "   macro avg       0.60      0.60      0.60        88\n",
      "weighted avg       0.67      0.68      0.68        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "              'penalty': ['l2']}\n",
    "\n",
    "# Ridge regularization\n",
    "ridge = LogisticRegression(penalty='l2', solver='lbfgs')\n",
    "ridge_cv = GridSearchCV(ridge, param_grid, cv=5)\n",
    "ridge_cv.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "# Evaluate models\n",
    "ridge_preds = ridge_cv.predict(X_test)\n",
    "ridge_acc = accuracy_score(y_test, ridge_preds)\n",
    "\n",
    "print('Ridge accuracy:', ridge_acc)\n",
    "# calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_test, ridge_preds,pos_label=0)\n",
    "recall = recall_score(y_test, ridge_preds,pos_label=0)\n",
    "f1 = f1_score(y_test, ridge_preds,pos_label=0)\n",
    "\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1 score: {:.2f}\".format(f1))\n",
    "\n",
    "# calculate AUROC\n",
    "auroc = roc_auc_score(y_test, ridge_preds)\n",
    "print(\"AUROC: {:.2f}\".format(auroc))\n",
    "\n",
    "#printing classification report\n",
    "print(classification_report(y_test,ridge_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec73e993",
   "metadata": {},
   "source": [
    "## Interpretation and Significance of output model and features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c635aa7",
   "metadata": {},
   "source": [
    ">**The final model (AdaBoost) achieved an F1 score of 0.68, indicating a good balance between precision and recall.**\n",
    "\n",
    ">**The Precision of 0.61 and Recall of 0.76 suggest that the model correctly identified 76% of the positive cases (true positives) and 61% of the predicted positive cases were actually positive (precision).**\n",
    "\n",
    ">**The AUROC score of 0.78 indicates that the model has good discriminative power to distinguish between positive and negative cases.**\n",
    "\n",
    ">**The classification report shows that the model has better performance in predicting negative cases than positive cases, which is reflected in the higher precision and lower recall for class 0.** \n",
    "\n",
    ">**The overall accuracy of the model is 0.80, which means that the model correctly classified 80% of the cases.**\n",
    "\n",
    ">**The weighted average F1 score of 0.80 indicates that the model performs well across both classes, with slightly better performance for class 1 due to its higher support.**\n",
    "\n",
    ">**In addition to the model performance, the significance of the features used in the model should also be considered.**\n",
    "\n",
    ">**This are the features are used in the final model :'M5', 'M25', 'M34', 'M53', 'M58', 'M63', 'M66', 'M72', 'M87', 'M124', 'M127', 'M148', 'M149', 'M150', 'M151'.**\n",
    "\n",
    ">**These features have been selected based on their importance and have shown to be relevant in predicting the target variable.** \n",
    "\n",
    ">**Therefore, we can conclude that the model is using the appropriate set of features and is giving good predictions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963d5f79",
   "metadata": {},
   "source": [
    "# Predicting the probability of target being 1 on Out of Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "3484e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and read the out of training data\n",
    "oot_data = pd.read_csv(r\"C:\\Users\\pooji\\Downloads\\OOT_data_Jenfi_assessment_070423.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "9921e19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#oot_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "88f5e5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     0\n",
       "M1             0\n",
       "M2             1\n",
       "M3            35\n",
       "M4             0\n",
       "              ..\n",
       "M147           1\n",
       "M148           0\n",
       "M149          20\n",
       "M150          31\n",
       "M151           2\n",
       "Length: 152, dtype: int64"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for null values\n",
    "oot_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "ea568d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     0\n",
       "M1             0\n",
       "M2             0\n",
       "M3             0\n",
       "M4             0\n",
       "              ..\n",
       "M147           0\n",
       "M148           3\n",
       "M149          17\n",
       "M150           7\n",
       "M151           1\n",
       "Length: 152, dtype: int64"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for infinity values\n",
    "np.isinf(oot_data).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "0470ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing infinity with null values\n",
    "oot_data.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "f3bb38a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping unnamed column ad target\n",
    "oot_data= oot_data.drop([\"Unnamed: 0\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "08fcd96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#oot_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6c7236",
   "metadata": {},
   "source": [
    "#### Scaling the data using Min-Max Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "45a474be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scale features\n",
    "scaler = MinMaxScaler()\n",
    "model=scaler.fit(oot_data)\n",
    "scaled_oot_data=model.transform(oot_data)\n",
    "\n",
    "# creating a data frame to put into KNN imputer\n",
    "oot_data=pd.DataFrame(scaled_oot_data,columns=oot_data.columns)\n",
    "#oot_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d3ac58",
   "metadata": {},
   "source": [
    "#### Imputing null values with KNN imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "96bf571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the KNNImputer class\n",
    "imputer = KNNImputer(n_neighbors=1)\n",
    "\n",
    "# Fit and transform the DataFrame to impute missing values\n",
    "oot_data_imputed = imputer.fit_transform(oot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "d772eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting imputed oot_data into dataframe\n",
    "oot_data_imputed = pd.DataFrame(oot_data_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "9822976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverse transform of Knn Imputed File\n",
    "oot_data_imputed=pd.DataFrame(scaler.inverse_transform(oot_data_imputed),columns=oot_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "c3b76760",
   "metadata": {},
   "outputs": [],
   "source": [
    "#oot_data_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "37709a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking the relevant features same as training data\n",
    "X_oot = oot_data_imputed[['M5', 'M25', 'M34', 'M53', 'M58', 'M63', 'M66', 'M72', 'M87', 'M124', 'M127', 'M148', 'M149', 'M150', 'M151']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4355b427",
   "metadata": {},
   "source": [
    "## Applied the final model on the Out-of-training data set (on relevant features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "f3f42a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.51493723 0.50557229 0.68496913 0.51994063 0.47345456 0.5039621\n",
      " 0.5074089  0.48374281 0.52483231 0.5208329  0.67770365 0.51410849\n",
      " 0.504516   0.48064784 0.51394513 0.51895337 0.50919354 0.5250877\n",
      " 0.52168849 0.67948684 0.50078394 0.6835133  0.50275284 0.50515446\n",
      " 0.49151058 0.50712372 0.50268093 0.51710876 0.693917   0.47690804\n",
      " 0.51583693 0.53033555 0.50603886 0.51959046 0.50908649 0.5014148\n",
      " 0.47922713 0.50187894 0.51564457 0.68822378 0.50406718 0.51242852\n",
      " 0.51463652 0.49783603 0.49393418 0.47098708 0.52388653 0.68122439\n",
      " 0.50215084 0.49822078 0.50351068 0.49548361 0.64585758 0.49916937\n",
      " 0.68045334 0.50411687 0.50748122 0.51379186 0.49604279 0.51448729\n",
      " 0.38620707 0.4967546  0.6631159  0.48351922 0.68369949 0.50876449\n",
      " 0.6905706  0.49710627 0.65959294 0.49943539 0.51787844 0.52586734\n",
      " 0.6839643  0.51483086 0.49811185 0.51583693 0.50238517 0.52047219\n",
      " 0.50384905 0.5110288  0.50128133 0.49488029 0.50993126 0.50634238\n",
      " 0.51780633 0.51375308 0.49053119 0.5050626 ]\n"
     ]
    }
   ],
   "source": [
    "# Train the AdaBoost model\n",
    "model = AdaBoostClassifier()\n",
    "model.fit(X_train_sm_vif, y_train_sm)\n",
    "\n",
    "# Use the trained model to predict the target probabilities for the dataset\n",
    "y_pred_prob = model.predict_proba(X_oot)[:, 1]\n",
    "\n",
    "# Print the predicted probabilities for each entry in the dataset\n",
    "print(y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "a6e82f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#length of y_pred_prob\n",
    "len(y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "3017695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting it into a dataframe\n",
    "y_pred_prob = pd.DataFrame(y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "f79486d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dataframe into excel file\n",
    "y_pred_prob.to_excel(\"Poojitha_Gujjula_070423_OOT_prediction.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30da9ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
